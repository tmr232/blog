[{"content":"Why? The first question you should ask yourself when seeing this title is \u0026ldquo;why?\u0026rdquo;. Why would we want to add wikilink1 support to a Markdown formatting library?\nWell, recently I started using Obsidian (a Markdown-based personal wiki). Since my writing is mostly code-related, it includes a lot of code-snippets. As I like having my code neatly formatted, and hate formatting it by hand, I wanted a tool to do that for me. The best tool I found was mdformat. It\u0026rsquo;s Python based, formats Python, Rust, and Go code snippets, and even formats the Markdown itself.\nThe only problem being: it formats a lovely wiki [[link]] as \\[\\[link\\]\\], breaking it in the process. To mitigate that, I had to add wikilink support2.\nIf you just want to see the code, go to mdformat-wikilink.\nMdformat Plugins mdformat is a markdown formatting tool written in Python. It is based on the wonderful markdown-it-py library, and has plugin support.\nThere are 2 types of mdformat plugins:\nCode formatter plugins, used for formatting the code inside fenced blocks; Parser extension plugins, used to add support for new nodes. Since we\u0026rsquo;re adding support for a new type of syntax, a wikilink, we\u0026rsquo;ll be writing a parser extension plugin. To do so, we\u0026rsquo;ll follow the mdformat guide for developing plugins.\nThat said, our mdformat plugin will only do the rendering. For parsing, we need to write a markdown-it-py plugin.\nMarkdown-it-py Plugins Luckily for us, markdown-it-py has very good support for plugins as well. Documentation includes design principles (which make for a good architecture overview), API documentation, and existing plugins. You can also check the markdown-it live demo (using the Javascript library that was later ported to Python) to interactively see a token stream.\nActual Code After a bit of reading, experimenting, and finding out - it seems that we only need very little code to make things work. We can do something more complex, but for our needs (ensuring mdformat doesn\u0026rsquo;t modify wikilinks) we can hack something quick. We\u0026rsquo;re going to create a new parser token for wikilinks, and make mdformat render it as-is.\nThe code will consist of 3 parts:\nmarkdown-it-py plugin, to parse the wikilinks as a new token mdformat plugin, to use the previous plugin, and render the new token as-is A bit of pyproject.toml config to make mdformat recognize the plugin. Parsing Wikilinks Since we\u0026rsquo;re only parsing wikilinks to keep them unmodified, we\u0026rsquo;re not going to break them up into parts. Instead, we\u0026rsquo;ll just keep them as a block of text. This means that we can write a simplistic parser that does the following:\nUsing regex, we check whether the string currently fed to the parser is a link If it isn\u0026rsquo;t, we do nothing and report that it isn\u0026rsquo;t. If it is, we push a wikilink token with the entire link as its content, and increment the parser position part the link. The last (and most important) part of the code is registering the parser we just wrote. We register it as an \u0026ldquo;inline\u0026rdquo; rule (as it is an inline element, not a block), and we register it last, as it doesn\u0026rsquo;t replace any other elements (well, plain text\u0026hellip;).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 import re from markdown_it import MarkdownIt from markdown_it.rules_inline import StateInline LINK_PATTERN = re.compile(r\u0026#34;\\[\\[([^[|\\]\\n])+(\\|[^]\\n]+)?]]\u0026#34;) def _wikilink_inline(state: StateInline, silent: bool) -\u0026gt; bool: match = LINK_PATTERN.match(state.src[state.pos :]) if not match: # Not a wikilink! return False # Push the wikilink token token = state.push(\u0026#34;wikilink\u0026#34;, \u0026#34;\u0026#34;, 0) token.content = match.group() # Increment parser location state.pos += match.end() # Found a wikilink! return True def wikilink_plugin(md: MarkdownIt) -\u0026gt; None: # Register the parser! md.inline.ruler.push(\u0026#34;wikilink\u0026#34;, _wikilink_inline) Formatting Wikilinks Our mdformat plugin is even more simplistic.\nThe update_mdit function is used to load our wikilink-parsing plugin into the current instance of markdown-it-py The _render_wikilink function returns the content of the wikilink token (or node, in mdformat terminology) that we pushed That\u0026rsquo;s it.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 from collections.abc import Mapping from markdown_it import MarkdownIt from mdformat.renderer import RenderContext, RenderTreeNode from mdformat.renderer.typing import Render from mdformat_wikilink.mdit_wikilink_plugin import wikilink_plugin def update_mdit(mdit: MarkdownIt) -\u0026gt; None: # Load the markdown-it wikilink plugin to parse wikilinks mdit.use(wikilink_plugin) def _render_wikilink(node: RenderTreeNode, context: RenderContext) -\u0026gt; str: # Render as-is. return node.content # Register the render function RENDERERS: Mapping[str, Render] = {\u0026#34;wikilink\u0026#34;: _render_wikilink} A Tiny Bit of Config The last thing we need to do is register the right entry-point for our plugin, so that mdformat will know to load and use it. We can do it in our pyproject.toml file (I\u0026rsquo;m using Poetry, other tools have similar options).\n1 2 [tool.poetry.plugins.\u0026#34;mdformat.parser_extension\u0026#34;] \u0026#34;wikilink\u0026#34; = \u0026#34;mdformat_wikilink.mdformat_plugin\u0026#34; And with that, we\u0026rsquo;re done.\nYou can see the whole project at mdformat-wikilink.\nWikilinks the link markup you see in wikis like Wikipedia. They are of the form [[Target]] or [[Target|Alias]], and generally create links inside the wiki.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAnd a blog post documenting it, for future reference. Which you are now reading.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://tamir.dev/posts/adding-wikilink-support-to-mdformat/","summary":"Why? The first question you should ask yourself when seeing this title is \u0026ldquo;why?\u0026rdquo;. Why would we want to add wikilink1 support to a Markdown formatting library?\nWell, recently I started using Obsidian (a Markdown-based personal wiki). Since my writing is mostly code-related, it includes a lot of code-snippets. As I like having my code neatly formatted, and hate formatting it by hand, I wanted a tool to do that for me.","title":"Adding Wikilink Support To Mdformat"},{"content":"Recently I was asked about Python tools \u0026amp; libraries that I use. After taking the time to write a lengthy reply, I figured I might as well write a blog post, to make it more widely available.\nThis post is going to go over some Python tools \u0026amp; libraries that I use and that I like.\nFormatting Automatic formatting gives you the gift of not ever caring about formatting again. Not when you resolve conflicts, not when you review code, not when you write code.\nI currently use 3 different formatters to get it done. I run autoflake, then isort, then black.\nBlack The uncompromising Python code formatter\nBlack is a code-formatter for Python with minimal (line length) configurable settings. It\u0026rsquo;s a really good code formatter, and will help avoid all the \u0026ldquo;but I like my code this way\u0026rdquo; complaints. Additionally, it\u0026rsquo;s now more-or-less the de-facto standard for formatting Python code.\nTo paraphrase Rob Pike, \u0026ldquo;Black\u0026rsquo;s style is no one\u0026rsquo;s favorite, yet Black is everyone\u0026rsquo;s favorite.\u0026rdquo;\nisort isort sorts your imports. It sorts them alphabetically, and also groups them separating standard-library, third-parties, and your own code.\nEven if you don\u0026rsquo;t care about sorting imports - it\u0026rsquo;s going to solve a ton of import-duplication \u0026amp; merge-conflicts for you.\nautoflake autoflake removes unused imports. That\u0026rsquo;s it. Use it.\nLinting Just like automatic formatting - you don\u0026rsquo;t need to think about anything you can lint for. This also means you never need to argue about it.\nmypy mypy is a static type-checker for Python. It uses type annotations \u0026amp; type inference to tell whether your types match or not. It is not perfect, but it catches often catches major issues in my code.\nI highly recommend using it in your project from the get-go. Writing types as you code is easy. Writing types for an existing project is exhausting.\nWhile the default settings are ok, there are some extras I recommend:\ncheck-untyped-defs makes sure you get warnings even if you didn\u0026rsquo;t annotate a function at all. This one, in particular, can be annoying to turn on late into a project. strict-equality warns you if you compare non-overlapping types. Those comparisons are usually bugs. warn-return-any warns if you return an unknown type from a function annotated to return a specific type warn-no-return warns if you only have return statements in some branches of your function. This is annoying sometimes, but saves you in the cases where you actually forget to return a value. warn-unreachable warns you when a piece of code is deemed unreachable. This one is super helpful in realizing that you made a typing error. Like checking a variable that\u0026rsquo;s never None against None. And the make the output friendlier, use:\npretty show-error-context pylint Pylint, with it\u0026rsquo;s catchy tag-line (\u0026ldquo;It\u0026rsquo;s not just a linter that annoys you!\u0026rdquo;), is for the most part just a linter that annoys you.\nIf you set it up, and disable all the errors you don\u0026rsquo;t care about (I have over 20 disabled globally), it can give some value. But it is also painfully slow.\nIt is currently here because I still use it, but I am more and more hesitant about including it in new projects. I just don\u0026rsquo;t feel like its value is worth the time and hassle. Especially with mypy catching a lot of the true errors pylint catches.\nflake8 flake8 is a style-guide-enforcer for Python. It is fast and capable, and has a large assortment of plugins that can catch actual bugs as well.\nRuff Ruff is \u0026ldquo;an extremely fast Python linter, written in Rust.\u0026rdquo; And it lives up to that. I don\u0026rsquo;t have much experience with it in production, so I don\u0026rsquo;t know how much it actually catches compared to the previous tools.\nRuff is now in very active development, and I plan to integrate it soon. Even if it\u0026rsquo;s missing some features you currently want, I think it\u0026rsquo;s worth keeping an eye out for it.\nTesting \u0026amp; Automation pytest Pytest is my test-framework of choice. The fixture-based design takes a while to get used to, and there is a bit of \u0026ldquo;magic\u0026rdquo; going on. But once you get the hang of it, it is extremely capable and easy to use and extend.\nnox nox makes it easy to automate your tests for multiple Python environments (think multiple versions of Python, multiple OSs, etc.).\nI find it straight-forward and easy to use and extend, as the configuration is entirely in Python.\nIn addition to tests, I use it to automate code formatting \u0026amp; linting, code-generation, and various CI tasks. That way I know that what I run locally and in the CI uses the same code and configuration.\nYou can also read Hynek Schlawack\u0026rsquo;s post on nox.\nProfiling \u0026amp; Benchmarking memray Memray is a memory profiler for Python. It does what it says on the tin, and does it well. It shows you which parts of your code allocated the most memory, and allows you to easily analyze that using multiple \u0026ldquo;reporters\u0026rdquo;. My most-used reporters are the Flame Graph Reporter and the Tree Reporter.\nThe only major downside is that it does not support Windows (unless you\u0026rsquo;re using WSL).\nAustin Austin is a sampling profiler for Python. It is fast and capable, and I use it a lot.\nBe aware that some related tools (austin-web) are not always as up-to-date and may cause issues.\nI recommend using it to profile an entire run, and then use Speedscope to analyze it. If you need to analyze a part of a run [austin-tui][] can show a live view, and then save it to a trace file.\nThe default output format for Austin is supported by Speedscope and is also easily editable using scripts (if you want to filter out specific parts of a run).\npy-spy py-spy is another sampling profiler for Python. For me, it\u0026rsquo;s main benefit is it\u0026rsquo;s top view, which gives a good overview of a process and lets you know \u0026ldquo;what\u0026rsquo;s taking so long?\u0026rdquo;. Additionally, it can show you stack traces for all the currently running threads.\nI usually use it when a running process misbehaves and I want to know why.\nThe Python Profilers The Python Profilers are for when sampling profilers aren\u0026rsquo;t enough. You need to know the call-count, and not just the durations. They don\u0026rsquo;t sample, so they will severely affect your program\u0026rsquo;s runtime. But there is no real alternative.\nFor viewing and analyzing the data, I highly recommend using KCacheGrind (or possible QCacheGrind if you\u0026rsquo;re on Windows and don\u0026rsquo;t wanna bother with setting up GUI for WSL). It is extremely fast even with very large profiles, and has some very good visualizations for analyzing the data. To convert the data to a fitting format, use pyprof2calltree\nMemory Usage Over Time Sometimes the simplest solution is the best one.\nWhen I need to find the part in my code that suddenly allocates way to much - I often log my memory usage over time, graph it, then compare it with a log or a sampling-profiler run to see which part of the run correlates with the spike in memory usage.\nIt\u0026rsquo;s crude, but it works.\nI tend to use the psutil library for it.\npytest-benchmark pytest-benchmark allows you to easily run short-benchmarks in your test-suite. It takes care of all the complicated stuff - repeating the runs, and calculating statistics - and gives you easy-to-read results.\nI also wrote a small tool on top of it to perform comparative-benchmarks and compare different implementations for the same code. You can find it at tmr232/python-benchmark.\npytest-json-report pytest-json-report generates a JSON report for a pytest run. It has 2 main benefits:\nMakes it easy to parse the test results and create custom reports Makes it easy to add extra information into the test results I use it to add peak-memory-usage into the test-results, so that I can keep track of that.\nPackaging \u0026amp; Deps Poetry Poetry is my go-to for managing my Python dependencies.\nThe main advantage of Poetry over other tools is that it automatically maintains a lock-file for you. By committing the lock-file into your repo, you ensure that package versions will be identical across all your environments. That way you know that what you develop with locally is the same as what\u0026rsquo;s tested in the CI, same as what\u0026rsquo;s deployed to staging, and also the same as what you deploy to production.\nI think that this alone should be enough to convince you.\nLibraries rich Rich is a Python library for rich text and beautiful formatting in the terminal.\nIf you print anything to the terminal - use Rich. It\u0026rsquo;s better than any other library in the category.\ntyper Typer, from the creator of FastAPI, makes building advanced CLIs easy and painless.\nIn it\u0026rsquo;s most basic - it takes the argument names and type annotations from a function, and converts it to a fully-features CLI (flags, arguments, documentation, completions).\nattrs attrs is my preferred way to write Python classes. It uses type annotations to declaratively define your classes, and does a fantastic work of deducing extra functionality from it (comparisons, equality, hash\u0026hellip;).\nTo keep things short - I don\u0026rsquo;t remember the last time I wrote an __init__ method.\nAltair If you ever need to draw a graph, use Altair.\nFor it\u0026rsquo;s philosophy and a brief intro, I recommend watching How to Think about Data Visualization by Jake VanderPlas.\nThe only downside I found so far is that exporting an image usually requires a web-browser in the process. But if your final output is HTML - you should be good.\nPandas Yes, pandas, the data-frame library.\nIf you\u0026rsquo;re ever dealing with numeric data, it\u0026rsquo;s worth spending the time (a couple of days, in my case) to develop basic competency with pandas. Once you have that, a lot of annoying tasks that you used to do in Excel become easy and straightforward.\n","permalink":"https://tamir.dev/posts/python-tools-and-libs/","summary":"Recently I was asked about Python tools \u0026amp; libraries that I use. After taking the time to write a lengthy reply, I figured I might as well write a blog post, to make it more widely available.\nThis post is going to go over some Python tools \u0026amp; libraries that I use and that I like.\nFormatting Automatic formatting gives you the gift of not ever caring about formatting again. Not when you resolve conflicts, not when you review code, not when you write code.","title":"Python Tools \u0026 Libraries"},{"content":"All whitespace is significant.\nIt might not always matter to your computer, or compiler, or piece of code. But to you, a human reading the code, it is significant.\nI often here people complaining about significant whitespace. They say it makes no sense, that it makes working with the code harder. That whitespace, specifically in code, should not be significant. But the unavoidable truth is that whitespace is always significant, regardless of the language you use.\nWhitespace Primer Before we talk about its significance, we need to define what whitespace is.\nWe\u0026rsquo;ll start with the Wikipedia definition of a whitespace character:\n[\u0026hellip;] any character or series of characters that represent horizontal or vertical space in typography. When rendered, a whitespace character does not correspond to a visible mark, but typically does occupy an area on a page.\nNext, we\u0026rsquo;ll divide it into 2 categories - visible and invisible.\nVisible Whitespace Visible whitespace is the whitespace you can see. Indentation, spacing, line breaks\u0026hellip; All of them make for visible whitespace.\nThe words in this line are separated by spaces. This line is indented using 4 spaces. An empty line precedes this line! Invisible Whitespace Invisible whitespace is all the whitespace you can\u0026rsquo;t see. This is not because it\u0026rsquo;s using different characters, but because it is positioned where it does not directly move other characters.\nThis line ends with 4 spaces. There are 2 line breaks after this line. Since you cannot see it, it\u0026rsquo;s hard to make sense of it. This is the source of many complaints.\nIndistinguishable Whitespace In addition to visible and invisible whitespace, there\u0026rsquo;s another category. Indistinguishable whitespace.\nThis category includes, for the most part, tabs and spaces.\nHere we indent with 4 spaces. Here we indent with a tab. Since they look the same, but are not the same character, they cause many issues.\nSignificance There are 2 ways for whitespace to be significant. It can be human-significant, meaning that it is significant for the reader; and it can be machine-significant, meaning that it matters to the computer.\nMost (if not all) whitespace complaints stem from disparity between those two concepts. From cases where whitespace is human-significant and not machine-significant, or vice versa.\nGoing forward, we\u0026rsquo;ll call situations where human- and machine-significance match \u0026ldquo;matched significance\u0026rdquo;, and cases where they do not \u0026ldquo;mismatched significance\u0026rdquo;.\nLet\u0026rsquo;s look at some examples.\nMatched Significance Plain Text This line has spaces it in. Lines are separated by line breaks. We can have multiple spaces. Or multiple line breaks. With the exception of invisible whitespace (trailing spaces or line breaks), there is no mismatch.\nPython 1 2 def f(): return 1 In Python, whitespace is significant for both the human and the machine in defining scopes. The second line is indented, marking it a part of the function defined on the first line.\nMismatched Significance Markdown 1 2 3 4 5 6 In Markdown, there\u0026#39;s some mismatch. The previous line ends with 2 significant spaces. This means that when rendered, it will remain a separate line, while the rest of the lines will get merged. This is not consistent across all variants, making it even worse. Since human readers cannot see the 2 trailing spaces, there\u0026rsquo;s a mismatch. We expect one output, but the computer gives us another.\nC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 static OSStatus SSLVerifySignedServerKeyExchange(SSLContext *ctx, bool isRsa, SSLBuffer signedParams, uint8_t *signature, UInt16 signatureLen) { OSStatus err; ... if ((err = SSLHashSHA1.update(\u0026amp;hashCtx, \u0026amp;serverRandom)) != 0) goto fail; if ((err = SSLHashSHA1.update(\u0026amp;hashCtx, \u0026amp;signedParams)) != 0) goto fail; goto fail; if ((err = SSLHashSHA1.final(\u0026amp;hashCtx, \u0026amp;hashOut)) != 0) goto fail; ... fail: SSLFreeBuffer(\u0026amp;signedHashes); SSLFreeBuffer(\u0026amp;hashCtx); return err; } This is Apple\u0026rsquo;s goto-fail bug and it is one of my favourite examples of significant whitespace. In line 12 there\u0026rsquo;s a goto fail statement. Due to the indentation (whitespace!) it reads (to the human) as if it belongs in the same block as line 11. But since indentation is insignificant whitespace in C, the computer ignores it.\nTo be more explicit in C\u0026rsquo;s syntax, we\u0026rsquo;ll write it as follows:\n1 2 3 4 if ((err = SSLHashSHA1.update(\u0026amp;hashCtx, \u0026amp;signedParams)) != 0) { goto fail; } goto fail; Making it clear that it will always goto fail.\nI like this example as it is a significant security issue that was (at least in part) caused by whitespace.\nIndistinguishable Hell So we know whitespace is significant. Both to humans and to machines. Taking a look at any code-formatters we\u0026rsquo;ll also see that people like it that way. If I put these 2 different formatting options for a vote, I\u0026rsquo;m pretty sure which one will win:\n1 2 3 4 5 6 7 8 9 void f(int x) { if (x \u0026lt; 10) { printf(\u0026#34;%d is smaller than 10.\\n\u0026#34;, x); else if { printf(\u0026#34;%d is 10.\\n\u0026#34;, x); } else { printf(\u0026#34;%d is larger than 10.\\n\u0026#34;, x); } } 1 2 3 4 5 6 7 8 9 void f(int x) { if (x \u0026lt; 10) { printf(\u0026#34;%d is smaller than 10.\\n\u0026#34;, x); else if { printf(\u0026#34;%d is 10.\\n\u0026#34;, x); } else { printf(\u0026#34;%d is larger than 10.\\n\u0026#34;, x); } } After all, none of us really want to count matching braces.\nSo why does \u0026ldquo;significant whitespace\u0026rdquo; get so much hate?\nWell, consider the following:\n1 2 3 4 # Python def f(): print(\u0026#34;Indented using spaces.\u0026#34;) print(\u0026#34;Indented using tab.\u0026#34;) 1 2 3 # makefile target: echo \u0026#34;Fail!\u0026#34; Both of these examples look valid, but they aren\u0026rsquo;t. To the naked (human-) eye, they are indistinguishable from valid code. But they mismatch spaces and tabs. Two indistinguishable types of whitespace.\nThis is, as mentioned before, the cause of most of the issues people have with whitespace. They expect it to work, but it doesn\u0026rsquo;t. In addition to that, there\u0026rsquo;s no meaningful or straightforward way to detect it when you look at the code. It\u0026rsquo;s an invisible problem.\nHate The Right Things I am not going to tell you to stop hating significant whitespace. Whitespace hurt you, and that anger needs to be directed somewhere. I will ask you, though, to point it in the right direction.\nVisible, distinguishable whitespace, with matching human- and machine-significance, is a good thing. It helps you make sense of the code, and helps ensure that the computer makes the same kind of sense of it as well.\nInvisible, yet machine-significant whitespace is bad. It leads to surprising outcomes and confuses the human writing the text.\nVisible, yet machine-insignificant whitespace is also bad. It leads to surprising outcomes and is tricky to detect.\nIndistinguishable, yet significant whitespace is the worst. It leads to bugs, errors, and pain. Use whatever tool is available in your toolbox to fight it. Use linters, formatters, and if all else fails - in-editor \u0026ldquo;visible whitespace\u0026rdquo; features. Avoid writing systems that allow it. And remember - the problem with Python is not that indentation is significant, it\u0026rsquo;s that tabs are allowed.\n","permalink":"https://tamir.dev/posts/significant-whitespace/","summary":"All whitespace is significant.\nIt might not always matter to your computer, or compiler, or piece of code. But to you, a human reading the code, it is significant.\nI often here people complaining about significant whitespace. They say it makes no sense, that it makes working with the code harder. That whitespace, specifically in code, should not be significant. But the unavoidable truth is that whitespace is always significant, regardless of the language you use.","title":"Significant Whitespace"},{"content":"Preface I\u0026rsquo;ve been wanting to write this post for years now. I did the research and had a (bad) draft of this lying around for years (since mid 2018, it seems). But I never felt that it was quite good enough.\nBlogvent was not a huge success in terms of post-quantity1 for me. I had some health issues that stopped me when I was getting started, and I found it hard to jump back in. That said, I see it as a very successful endeavor. I finally got my website up, and I feel more comfortable about posting imperfect posts.\nSo here goes.\nMoving to LA In The Karate Kid (the 1984 one), Lucille LaRusso \u0026amp; her son move from Newark, New Jersey, to Reseda, Los Angeles, California. From that point on, the movie focuses almost entirely on her son, Daniel.\nThe movie opens with Lucille LaRusso \u0026amp; her son, Daniel, moving from New Jersey to California. This move sets up the entire movie, which later follows Daniel as he adapts to his new environment. Despite that, the movie spends almost no time explaining the move.\nThis makes sense, as Daniel is the protagonist, and the teen audience of the movie probably don\u0026rsquo;t really care about Lucille\u0026rsquo;s motivation. This means that we only get a few small glimpses into the reasoning behind the move.\nIn this post, we\u0026rsquo;ll dig into that move.\nRocket Computers We first hear about the LaRusso\u0026rsquo;s reason for relocating when they unpack their car at their new place. Daniel meets Freddy, and they talk (5 minutes into the movie):\n1 2 3 4 5 6 7 Freddy: Where you from? Daniel: New Jersey. Freddy: Wow! Whatcha doin\u0026#39; here? Daniel: My mom got a new job with a company working out here. Rocket computers. Flight of the future. Freddy: Never heard of it. Daniel: It\u0026#39;s up and coming. So now we know - they moved because Lucille got a fancy new job. For a long time, I was happy with this explanation. Sadly, it doesn\u0026rsquo;t hold up.\nManager Training You see, later in the movie (23 minutes into the movie), Daniel meets his mother for lunch—at the restaurant where she works. This is already suspicious—wasn\u0026rsquo;t she working with computers?\nAdditionally, she\u0026rsquo;s raving about the relative benefits of working there!\n1 2 3 4 5 6 7 Lucille: Guess what? I\u0026#39;m going to be trained as a manager. Isn\u0026#39;t that great? Daniel: Yeah. Lucille: They have this program: two nights a week. As soon as a spot opens, you\u0026#39;re in. And the benefits?! I\u0026#39;d never get them working in computers. They pay for EVERYTHING. Now, this made no sense to me. Sure, it\u0026rsquo;s the 80\u0026rsquo;s, but being a waitress or manager can\u0026rsquo;t be better than working in computers, can it? So I went ahead and did some research.\nCrunching Numbers I was lucky enough to chance upon the Bureau of Labor Statistics\u0026rsquo; Occupational Outlook Handbook of 1988. It gives the median earnings of 1986 for some potentially relevant professions:\nRestaurant Managers make $22,400 a year Computer Programmers make $27,000 Computer Operators make $16,500 Computer Systems Analysis make $32,800 While the pay for a waitress (according to weekly earnings in 1983 and weekly earnings in 1985) is around $8000 (weekly earning of $152 to $159 multiplied by 52 weeks a year).\nDepending on which computer-related profession Lucille had, she may make more money as a restaurant manager, but the waiter pay makes a transition entirely unreasonable.\nThis makes no sense.\nThe Missing Scene! This bugged me for years. Literally. But today, while looking at the The Karate Kid screenplay to make sure I get the quotes right, it finally made sense! While going over the script looking for \u0026ldquo;computer\u0026rdquo; references, I came upon a scene I did not recognize. A scene that never made it into the movie! In that scene, Lucille sheds light onto her changing jobs:\n1 2 3 4 5 6 Lucille: I got a job. Daniel: Yeah. I know. Rocket Computers. Flight to the future. Lucille: Crash landed in the present is more like it. They went bankrupt last Friday. Can you believe it? That\u0026rsquo;s the scene that I was missing! The reason behind her switching jobs!\nThis was supposed to happen right before Miyagi comes in to fix the faucet (I frantically scanned through the movie again to make sure I did not somehow forget the scene existing in the movie). I\u0026rsquo;m really happy having found this. It\u0026rsquo;s been bothering me since forever and it\u0026rsquo;s nice to know that it actually was thought through.\nIn addition to that, we get to learn that this is probably Lucille\u0026rsquo;s first computer job after taking night classes, which would explain why becoming a waitress (at least temporarily) made more sense than finding another computer job:\n1 2 3 Daniel: So after going to school and all those night classes \u0026#39;n stuff for computers, you\u0026#39;re going to be a waitress? In Conclusion The Karate Kid is a fantastic movie. It\u0026rsquo;s still one of my favourites and it holds up amazingly well. I am delighted that completing this post allowed me to solve the mystery of Lucille\u0026rsquo;s job change.\nI hoped you enjoyed it too!\nBlogvent Posts\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://tamir.dev/posts/karate-kid/","summary":"Preface I\u0026rsquo;ve been wanting to write this post for years now. I did the research and had a (bad) draft of this lying around for years (since mid 2018, it seems). But I never felt that it was quite good enough.\nBlogvent was not a huge success in terms of post-quantity1 for me. I had some health issues that stopped me when I was getting started, and I found it hard to jump back in.","title":"The Karate Kid"},{"content":"This Sunday, during the monthly meeting of the Israeli WG21 National Body discussion forum1, we discussed a paper by Yehezkel Bernat. The paper discusses a specific issue with C++ ranges and iterators. It demonstrates it with the following code sample:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;ranges\u0026gt; #include \u0026lt;iostream\u0026gt; namespace rv = std::views; int main() { for (auto i : rv::iota(0) | rv::filter([](auto i) { return i \u0026lt; 11; }) | rv::take(11)) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } The code in the example does the following:\nCreate an infinite2 range counting from 0 upwards (rv::iota(0)) Filters that to only return numbers smaller than 11 (rv::filter(...)) Takes the first 11 numbers - so 0 through 10 (rv::take(11)) Loops over those and prints them. But there is a problem.\nThe Problem Instead of printing all 11 numbers and terminating, it prints them, and then keeps running indefinitely3. This happens because after taking the 11th element, it also tries to increment towards the 12th. Since the range is not exhausted, iota keeps running through numbers. Those numbers keep get filtered out by filter as they are all 11 or larger, and so we never terminate.\nIf you come from other languages (like Python or Java), this should surprise you. In those other languages, similar code will work perfectly well.\nThis happens due to the design of iterators in C++.\nC++ Iterators C++ has many types of iterators. The simplest one being an input-iterator. For us to iterate over it, we need 2 operators4:\n*it, to get a value from the iterator; ++it, to increment the iterator. When iterating, we start with it pointing to the first element of our sequence. This means that we first use *it to get the current value, and only then use ++it to advance.\nThat\u0026rsquo;s why we had an issue in our above example - we read the 11th value, then tried to advance the iterator to an element that will never exist.\nAdditionally, we can\u0026rsquo;t skip the increment. If we do, a subsequent read from the iterator will repeat the current value.\nOther Languages The iterator design in C++ is significantly different from other languages. While C++ does read-then-increment, the common design in other languages is increment-then-read.\nPython uses __next__() to get the next value of an iterator, raising StopIteration if none exist.\nJava uses hasNext() to check if a value exists (and advance to it), then next() to get the value.\nIn both cases, there\u0026rsquo;s no reason to advance after we got the value we want. As a result - the issue won\u0026rsquo;t reproduce.\n1 2 3 4 5 6 7 8 9 from itertools import islice, count for i in islice( # This is the equivalent of rv::take filter( lambda x: x \u0026lt; 11, count(0) ), 11): print(i) Further Thoughts I don\u0026rsquo;t know the reason behind C++\u0026rsquo;s design. When looking at it, it feels like a derivative of the C-style loop,\n1 for (int i = 0; i \u0026lt; 10; ++i) { ... } Where the increment happens after each iteration. To me, this also seems consistent with the design of the range-based for loop in C++. And while it makes sense for iterating over pre-existing data, it feels a bit off to me when iterating calculated (or fetched) data. In those cases, Python and Java\u0026rsquo;s design feels more appropriate to me.\nAnother thing that comes to mind is Arno Schödl\u0026rsquo;s talk Why Iterators Got It All Wrong from CppNorth 2022. It discusses the design of iterators and how they mix up pointing to elements (begin() points to the first element) and borders (end() point after the last element). It seems to me that if begin() were to point before the first element (and therefore need to be incremented before being dereferenced) the issue would be resolved. Then again, I\u0026rsquo;m probably missing something.\nI recommend that you watch the talk and see what you think.\nQuite a mouthful, I know. And WG21 is the ISO C++ standard. You can see the agenda for said meeting here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn theory. Eventually the number will overflow.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAgain, in theory.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe need more, but we\u0026rsquo;ll ignore them for simplicity.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://tamir.dev/blogvent/2022-12-08/","summary":"This Sunday, during the monthly meeting of the Israeli WG21 National Body discussion forum1, we discussed a paper by Yehezkel Bernat. The paper discusses a specific issue with C++ ranges and iterators. It demonstrates it with the following code sample:\n1 2 3 4 5 6 7 8 9 10 11 #include \u0026lt;ranges\u0026gt; #include \u0026lt;iostream\u0026gt; namespace rv = std::views; int main() { for (auto i : rv::iota(0) | rv::filter([](auto i) { return i \u0026lt; 11; }) | rv::take(11)) std::cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39;\\n\u0026#39;; } The code in the example does the following:","title":"Thoughts on Iterators in Python and C++"},{"content":"I always knew image alt-text existed. I knew about it from learning HTML - it\u0026rsquo;s the text that shows when the image doesn\u0026rsquo;t load. And it\u0026rsquo;s also on XKCD as the extra pun (TIL that\u0026rsquo;s actually title= and not alt=). It\u0026rsquo;s only recently, when Twitter added image descriptions1 and my feed started talking about them, that I realized how important they are for accessibility.\nSince then, I\u0026rsquo;ve been making an effort to describe the images in all my tweets to the best of my ability. It\u0026rsquo;s not always easy (try describing control-flow graphs and explaining the differences between them,) but it is definitely worth it.\nYesterday I blogged for the first time in a long while. As I added images to the post, I realized that I don\u0026rsquo;t know how to properly describe images in Markdown. Until that day - I\u0026rsquo;ve never really done that.\nMarkdown Alt Text The Markdown2 image syntax consists of 3 parts:\n1 ![alt text](url \u0026#34;title\u0026#34;) That will (usually3) be rendered as:\n1 \u0026lt;img src=\u0026#34;url\u0026#34; alt=\u0026#34;alt text\u0026#34; title=\u0026#34;title\u0026#34;/\u0026gt; This is simple enough, and even allows us to write long-form alt text:\n1 2 3 4 5 6 ![This image doesn\u0026#39;t exist. I am only writing this to show that we can have really long alt-text. It can even span multiple lines as long as there are no blank lines in it.][long-alt-text] [long-alt-text]: my-image-url This also benefits us as we\u0026rsquo;re writing - we have a description of the images we use, in our text-only editors!\nAnd we should all use it to improve our posts and make them more accessible. I still need to go over my past posts and properly describe the images in them.\nFuture Ideas While learning a bit on this, I searched for \u0026ldquo;markdown accessibility\u0026rdquo;. This lead me an article about improving the accessibility of my markdown. It seems that there\u0026rsquo;s quite a lot I need to fix in my blog. So that\u0026rsquo;s one thing I need to figure out.\nGenerally, the things to fix fall into 2 categories: authoring and rendering. Rendering is purely a technical fix - I need to ensure my blog theme is accessible by changing the templates. The authoring part is a bit tricker. It\u0026rsquo;s too easy for me to miss alt-text here and there, or have multiple links with the same name. So for this - I think I\u0026rsquo;ll have to write a linter.\nFrom the Twitter docs, saved to the archive.org so that it doesn\u0026rsquo;t disappear:\nHow to add image descriptions How to write great image descriptions \u0026#160;\u0026#x21a9;\u0026#xfe0e; When talking about Markdown I\u0026rsquo;ll try to refer to CommonMark unless stated otherwise.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn this blog, for example, I display the title using \u0026lt;figure\u0026gt; and \u0026lt;figcaption\u0026gt; tags.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://tamir.dev/blogvent/2022-12-02/","summary":"I always knew image alt-text existed. I knew about it from learning HTML - it\u0026rsquo;s the text that shows when the image doesn\u0026rsquo;t load. And it\u0026rsquo;s also on XKCD as the extra pun (TIL that\u0026rsquo;s actually title= and not alt=). It\u0026rsquo;s only recently, when Twitter added image descriptions1 and my feed started talking about them, that I realized how important they are for accessibility.\nSince then, I\u0026rsquo;ve been making an effort to describe the images in all my tweets to the best of my ability.","title":"Markdown Images \u0026 Accessibility"},{"content":"When I started my professional career I was not doing software engineering, but rather reverse-engineering. Taking a compiled binary, disassembling it, and trying to understand what it does. My job was essentially reading code, with the stated goal in my job description was \u0026ldquo;vulnerability research\u0026rdquo;. Since then, I moved to the other side, and am now doing \u0026ldquo;forward engineering\u0026rdquo;.\nWhile it is very tempting to say that as programmers (or developers, or software engineers, or whichever term you\u0026rsquo;re comfortable with) we only write code, we know that this is not true. If fact, we spend a lot of our time reading code. Be it code reviews, reading docs, debugging, or finding that one line of code we need to stick in someone else\u0026rsquo;s face and say \u0026ldquo;See!?!?!?! I told you!!!\u0026rdquo;.\nReading code is a vital part of any code-related role. Be it forward- or reverse-engineering. That said, the perspective if very different. I see that difference when talking to my friends about code, some of them being developers and other reverse-engineers. But I also see it in how I approach my tasks. My approach to reading code is dramatically different between development and reverse-engineering tasks.\nPart of it is the difference in goals - as a software developer, I want the code to work well. I am not (usually) looking for bugs, because I really don\u0026rsquo;t wanna find them. As a reverse-engineer bugs are what we\u0026rsquo;re looking for. This leads to a different perspective. In addition to that, as a developer I tend to let my hubris control me, assuming that I can write bug-free code. That I can understand the entire system. When researching code for vulnerabilities, the assumptions are different. First and foremost - all systems have bugs. We just need to find them. And second - systems are big. Too big. This means that we need to focus, get to the interesting parts of an unknown system as fast as possible.\nThis, in my opinion, also leads to very different tools.\nThe developer\u0026rsquo;s tool of choice is a code editor. Be it vi, or emacs, or VSCode, or any other editor. We read the code as text, and process it as text. The editor might give as some colors or linking, but its still text. When we want to know whether 2 functions are similar, we read them both. When we want to know if a function has a complex flow - we read it.\na truncate in Go, as rendered by the Goland IDE The reverse-engineer\u0026rsquo;s tool of choice is often IDA (or Binary Ninja or Ghidra or Hopper or something similar). While those tools do allow reading the disassembled code as text, they also offer an additional view - a graph view.\nsame function, in IDAs graph view With a bit of training and getting used to it, the Graph view allows a reverse-engineer to quickly discern the flow of the function. With green and red arrows denoting the true and false branches of a condition, blue lines denoting unconditional jumps, and bold lines for backlinks (usually loops).\nWhile the graph does not convey all the information about a function, it still conveys a lot. It makes it easy to see if two functions are similar, or if a function is complicated.\nI like this ability, and I miss it dearly in my developer tooling. So I started playing around with graph-visualization of functions in Go, to see how it turns out. Using the least amount of code I could - taking advantage of Go\u0026rsquo;s SSA1 libraries, and graphing using GraphViz - I managed to create something that, while initial, I\u0026rsquo;m very happy with. The project is called go-overview-graph and renders source-code and graphs side-by-side. Here are some example graphs, because they are so pretty! I highly recommend going over the website and having a look yourself!\nSingle Static Assignment\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://tamir.dev/blogvent/2022-12-01/","summary":"When I started my professional career I was not doing software engineering, but rather reverse-engineering. Taking a compiled binary, disassembling it, and trying to understand what it does. My job was essentially reading code, with the stated goal in my job description was \u0026ldquo;vulnerability research\u0026rdquo;. Since then, I moved to the other side, and am now doing \u0026ldquo;forward engineering\u0026rdquo;.\nWhile it is very tempting to say that as programmers (or developers, or software engineers, or whichever term you\u0026rsquo;re comfortable with) we only write code, we know that this is not true.","title":"A Bit of Codeviz"},{"content":"So a bit of a last minute decision, but here goes!\nInspired by Ólafur Waage\u0026rsquo;s post, I decided to try and follow suit. After all - I\u0026rsquo;ve not blogged for about a year and a half, and that\u0026rsquo;s way too long.\nSo, blatantly copying Ólafur:\nRules One post every day in December. Each post has to be about tech/code in some way. Each post has to be longer than 4 tweets. (1120 characters) Posts don’t have to be revolutionary (this is a writing exercise, it’s ok to write a basic tutorial about if statements). Posts have to be written that day, you can’t stock up. Posts can be a continuation of Yesterday’s post (again, this is mainly a writing exercise). No edits or people reviewing your drafts (writing exercise, etc) ","permalink":"https://tamir.dev/posts/blogvent-calendar-2022/","summary":"So a bit of a last minute decision, but here goes!\nInspired by Ólafur Waage\u0026rsquo;s post, I decided to try and follow suit. After all - I\u0026rsquo;ve not blogged for about a year and a half, and that\u0026rsquo;s way too long.\nSo, blatantly copying Ólafur:\nRules One post every day in December. Each post has to be about tech/code in some way. Each post has to be longer than 4 tweets.","title":"Blogvent Calendar"},{"content":"TL;DR Graph memory-usage over time, correlate with logs, profit.\nOverconfidence Recently, I had to reduce the memory consumption of a Python process that became entirely unreasonable. Now, a while back I wrote about finding memory leaks in Python. I was pleased with myself and sure that with the knowledge I gained then, I can surely get this done!\nAnd oh, was I wrong\u0026hellip;\nHarsh Reality You see, both pympler and tracemalloc are wonderful tools. But like all tools, they have limitations. When you have a long-running (days) process with many (hundreds of millions) objects, the memory and performance costs of your tools add up quite significantly. Waiting for pympler to query all objects takes forever, and following references is completely impractical; viewing tracemalloc statistics is nice, but doesn\u0026rsquo;t help you narrow things down enough.\nSo, after 2 weeks of zero-to-minimal improvements (though I was sure I\u0026rsquo;m on the right track) I decided to try a different approach to understanding the memory usage of my code.\nTo The Rescue Enter memlog.py.\nmemlog is a simple, naive tool. It tracks the overall memory usage on a machine, and logs it (with a timestamp) to a CSV. That\u0026rsquo;s it. While the recorded data may include significant noise, running your code (\u0026amp; memlog) inside a container can reduce it significantly. Also, background memory noise tends to be insignificant when your process hogging all of your memory\u0026hellip;\nSo, I ran my process (with logs), ran memlog, and plotted a memory-over-time graph: And oh, oh no.\nInsight Looking at the graph, we can divide it into 3 parts:\nA near-instant rise at the beginning. This is by far the bulk of the memory-usage increase; A slow, gradual increase over the entire time-scale; A near-instant drop in memory-usage. Those parts are basically:\nLoading the data-set and various initialization; The bulk of the processing; Program termination. And for the past 2 weeks I\u0026rsquo;ve been busy reducing the memory-usage of\u0026hellip; the second part. Being absolutely sure it\u0026rsquo;s the most significant.\nSo yeah, that hurt. But only for a short time. For you see, with this newfound knowledge I could safely focus on the first few minutes of execution and disregard the rest for the time being.\nTrue. I\u0026rsquo;ll have to test the whole thing once I\u0026rsquo;m make any significant changes. Memory-usage might spike at a later point. Memory-optimization may cause performance degradation. But unless I reduce that uptick at the beginning I won\u0026rsquo;t get any significant improvements.\nProfit A week later, we managed to reduce memory-usage by 30% while reducing overall processing time by a similar percentage. We had to:\nAdd a de-duplicating weakref based cache; Add a pre-processing step; Make our code more cache-friendly by sorting our data; Remove a massively over-engineered control mechanism. But it was all made possible by focusing on the right part. Had I not plotted that memory graph, I could\u0026rsquo;ve easily spent another 2 weeks without any significant progress.\nOld \u0026amp; Wise So whatever you do, I highly suggest you graph your data. No need to be smart about it. Log it, graph it, correlate to your logs.\n","permalink":"https://tamir.dev/posts/more-memory-profiling-in-python/","summary":"TL;DR Graph memory-usage over time, correlate with logs, profit.\nOverconfidence Recently, I had to reduce the memory consumption of a Python process that became entirely unreasonable. Now, a while back I wrote about finding memory leaks in Python. I was pleased with myself and sure that with the knowledge I gained then, I can surely get this done!\nAnd oh, was I wrong\u0026hellip;\nHarsh Reality You see, both pympler and tracemalloc are wonderful tools.","title":"More Memory Profiling (in Python)"},{"content":"In writing our CI setup at Vdoo, we came across some interesting challenges. Having solved them and used the solutions for quite a while, we decided it is best to share, and hopefully save others some time and effort solving similar problems.\nOf course, there are alternative solutions to the challenges we have dealt with, and some solutions are probably superior to ours. We are happy to hear about such solutions and to improve our own.\nAs for the code presented in this post - it was extracted from our CI and cleaned up a bit. As such, it is missing some necessary boilerplate. It will not work as-is, and some work will be required to adapt it to your CI. That said, it should clearly lay out the solutions. (You can think of it as slide-ware.)\nLFS-Check All transitions can be bumpy. For us, the transition from storing binary files as regular git blobs to storing them using LFS was one such bumpy transition.\nWe made sure to include all the LFS-relevant files \u0026amp; patterns in a .gitattributes file, but ensuring everyone (including people who only occasionally work on the relevant repo) properly setup their environments for LFS took some time. In the mean-time, we kept getting files that should be in LFS committed and pushed as regular files in Merge-Requests.\nTo circumvent that, we set up a simple check at the start of our CI process to ensure all relevant files are indeed stored in LFS.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 lfs-check: only: refs: - merge_requests script: - git lfs install - git add --renormalize -u - | if ! git diff --cached --name-only --exit-code ; then echo echo echo \u0026#34;==============================\u0026#34; echo \u0026#34;# Please renormalize files #\u0026#34; echo \u0026#34;==============================\u0026#34; echo echo \u0026#34;git add -u --renormalize\u0026#34; echo \u0026#34;git commit --amend\u0026#34; exit 1 fi When people pushed the files the wrong way - the CI would fail with an informative error and instructions.\nRequired Commit Every so often a change is made to the code, rendering the code before the change unworkable or irrelevant. This can happen for many reasons. Here are some examples:\nA bug was fixed in the CI. This is all too common when forgetting to properly lock your dependencies (including recursive ones!) A very time-consuming update was made (re-training an ML model, anyone?) The change is significant and will make rebasing a pain A significant bug was fixed, making tests on the previous versions mostly irrelevant Once you introduce such a change to your code, you want people to know about it, and you want to stop wasting cycles on it.\nTo achieve this goal, we created a required-commit mechanism in our CI. For the CI to work, the required-commit must be an ancestor of the current commit. If it isn\u0026rsquo;t - the CI fails with a descriptive error \u0026amp; instructions for fixing the issue.\nOnce we have a new required-commit, we inform all developers in a dedicated Slack channel and update the CI to match. This ensures that even if a developer misses the notification on Slack, the CI will let them know what needs to be done.\nThe solution consists of a simple CI job:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 required-commit: only: refs: - merge_requests script: - apt-get update - | if ! git merge-base --is-ancestor ${REQUIRED_COMMIT:-HEAD} HEAD ; then echo echo echo \u0026#34;=============================\u0026#34; echo \u0026#34;# Rebase Required #\u0026#34; echo \u0026#34;=============================\u0026#34; echo echo \u0026#34;Your base commit is out of date.\u0026#34; echo \u0026#34;Please update to ${REQUIRED_COMMIT} or later.\u0026#34; echo \u0026#34;The easiest fix is to rebase-onto or merge-from origin/main.\u0026#34; echo echo exit 1 fi And a custom variable defined in the CI settings (see Create a custom variable in the UI):\nConditionally Building Job Docker Images Some of our code is deployed via Docker images. As such - we want our CI to build and test those images. Some tests require running a Docker container and communicating with it, but some tests (especially unit- and integration-tests) are easier to run inside the said containers. To accommodate the latter, we use our Docker images as the base images for the CI test jobs.\nThis is easy enough to do in the CI. In our case, however, building the Docker images takes a very long time. In trying to reduce this time, we split our build into two parts. The first - a long compilation phase, building some rarely-changing code; the second - installation of our fast-changing Python code \u0026amp; all relevant dependencies.\nNoticing the split between the fast-changing and rarely-changing parts of our build, we decided to split it in half, only building the first part when there\u0026rsquo;s an actual change to it.\nTo do that, however, we have to conditionally build the Docker image for the first half, and in the second half use either the preexisting first half or the newly built one.\nThe Solution - Build, Proxy, Promote Our solution uses a model consisting of multiple CI jobs handling different parts.\nBuild jobs - responsible for building docker images. Either conditionally (for the first part) or consistently (for the second part). Proxy jobs - responsible for handling the conditional nature of the build jobs, providing the next job with the relevant tag for the Docker images - either :latest or the current commit. Promote jobs - responsible for tagging the newly built images with :latest and pushing them. They run last. For our use-case, we used the following setup:\nConditional Build job to build the rarely-changing code Proxy job to yield the relevant tags Build job to build \u0026amp; install the fast-changing code Test job, to test the newly build code Promote job, pushing the newly build images as :latest if the tests passed To implement it, we created the following .yml configuration, representing the build-proxy-promote model, and used include:file to bring it into our .gitlab-ci.yml.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 # This file allows creating a prebuild-proxy-(build)-promote workflow with ease. # # The idea is that that in the prebuild step we build less-frequently-changed # docker images than in the build step. This allows us to significantly speed # up CI times. # # Setting Up # ========== # # A basic setup consists of the following: # # 1\\. Prebuild (extends .bpp:build) - build docker images if relevant files changed # 2\\. Proxy (extends .bpp:proxy) - allows the rest of the CI to know whether Prebuild created new images or not # 3\\. Build [Optional] (extends .bpp:build) - builds extra, more-frequently-changing images. # This is not a conditional step! # 4\\. Use (custom step) - here we actually use the images we created! # 5\\. Promote (extends .bpp:promote) - if required, pushes the newly built images to the project\u0026#39;s repository. # # These 5 steps should be in 5 different, consecutive stages for things to work. # The prebuild step, being conditional, should not have any other step requiring it. # All other steps (that need the prebuilt images) should require the proxy step instead, # and use the ${PROXY_TAG} to as a label to the relevant docker images. .bpp:build: variables: # The names of all the docker images we want to pull from our registry TO_PULL: \u0026#34;\u0026#34; # The tag to use for pulling the images. This will usually be ${PROXY_TAG} PULL_TAG: \u0026#34;\u0026#34; # The name of the image and path of the dockerfile for building docker images. # The root path for the dockers will be the root of the project # Format the variable as follows: # # \u0026gt;- # \u0026#34;some_name the/relevant/path/Dockerfile\u0026#34; # \u0026#34;some_other_name another/relevant/path/Dockerfile\u0026#34; # # Note that the quotes are significant! TO_BUILD: \u0026#34;\u0026#34; # The names of the images we want to push. # They will all be pushed with the ${CI_COMMIT_SHA} tag. TO_PUSH: \u0026#34;\u0026#34; script: - docker login -u ${CI_REGISTRY_USER} -p ${CI_REGISTRY_PASSWORD} ${CI_REGISTRY_IMAGE} - export DOCKER_BUILDKIT=1 # This cannot be in the `variables` field since users overwrite it. - | for IMAGE_NAME in ${TO_PULL} do echo \u0026#34;***********************************\u0026#34; echo \u0026#34;Pulling ${IMAGE_NAME}\u0026#34; echo \u0026#34;-----------------------------------\u0026#34; echo \u0026#34;docker pull ${CI_REGISTRY_IMAGE}/${IMAGE_NAME}:${PULL_TAG}\u0026#34; echo \u0026#34;DOCKER_BUILDKIT=1 docker tag \\ ${CI_REGISTRY_IMAGE}/${IMAGE_NAME}:${PULL_TAG} \\ ${CI_REGISTRY_IMAGE}/${IMAGE_NAME}:latest\u0026#34; echo \u0026#34;***********************************\u0026#34; docker pull ${CI_REGISTRY_IMAGE}/${IMAGE_NAME}:${PULL_TAG} docker tag \\ ${CI_REGISTRY_IMAGE}/${IMAGE_NAME}:${PULL_TAG} \\ ${CI_REGISTRY_IMAGE}/${IMAGE_NAME}:latest done - | eval \u0026#34;ARRAY=($TO_BUILD)\u0026#34; for ITEM in \u0026#34;${ARRAY[@]}\u0026#34; do MY_NAME=${ITEM% *} MY_PATH=${ITEM#* } echo \u0026#34;***********************************\u0026#34; echo \u0026#34;Building ${MY_NAME} from ${MY_PATH}\u0026#34; echo \u0026#34;-----------------------------------\u0026#34; echo \u0026#34;DOCKER_BUILDKIT=1 docker build \\ --build-arg BUILDKIT_INLINE_CACHE=1 \\ -t ${CI_REGISTRY_IMAGE}/${MY_NAME} \\ -t ${CI_REGISTRY_IMAGE}/${MY_NAME}:${CI_COMMIT_SHA} \\ -f ${MY_PATH} \\ --label \u0026#34;commit_sha=${CI_COMMIT_SHA}\u0026#34; \\ .\u0026#34; echo \u0026#34;***********************************\u0026#34; docker build \\ --build-arg BUILDKIT_INLINE_CACHE=1 \\ -t ${CI_REGISTRY_IMAGE}/${MY_NAME} \\ -t ${CI_REGISTRY_IMAGE}/${MY_NAME}:${CI_COMMIT_SHA} \\ -f ${MY_PATH} \\ --label \u0026#34;commit_sha=${CI_COMMIT_SHA}\u0026#34; \\ . done - | for IMAGE_NAME in $TO_PUSH do echo \u0026#34;***********************************\u0026#34; echo \u0026#34;Pushing ${IMAGE_NAME}\u0026#34; echo \u0026#34;-----------------------------------\u0026#34; echo \u0026#34;DOCKER_BUILDKIT=1 docker push ${CI_REGISTRY_IMAGE}/${IMAGE_NAME}:${CI_COMMIT_SHA}\u0026#34; echo \u0026#34;***********************************\u0026#34; docker push ${CI_REGISTRY_IMAGE}/${IMAGE_NAME}:${CI_COMMIT_SHA} done .bpp:proxy: variables: # The names of jobs we want to proxy - if any of them succeeded, we proxy. BUILD_JOBS: \u0026#34;\u0026#34; script: - PROXY_TAG=latest - apt-get -qq update - apt-get -qq install jq # Get the successful jobs for the current pipeline - \u0026gt;- curl --header \u0026#34;PRIVATE-TOKEN:${GITLAB_TOKEN}\u0026#34; \u0026#34;${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/pipelines/${CI_PIPELINE_ID}/jobs?scope[]=success\u0026#34; \u0026gt; jobs.json # Compare the job names from the pipeline with the provided job names - EXECUTED=$(comm -12 \u0026lt;(jq -r \u0026#39;.[].name\u0026#39; jobs.json | sort) \u0026lt;(echo ${BUILD_JOBS} | tr \u0026#39; \u0026#39; \u0026#39;\\n\u0026#39; | sort)) # If a build job was executed, we need to set the proxy tag to the current commit sha. - | if [ ! -z \u0026#34;$EXECUTED\u0026#34; ] then PROXY_TAG=${CI_COMMIT_SHA} fi - echo \u0026#34;PROXY_TAG=${PROXY_TAG}\u0026#34; \u0026gt;\u0026gt; deploy.env # Print out the proxy tag - for debug purposes - echo \u0026#34;PROXY_TAG=${PROXY_TAG}\u0026#34; artifacts: # To get the proxy tag, you need to get the artifacts from this job. # The proxy tag will be ${PROXY_TAG} reports: dotenv: deploy.env .bpp:promote: variables: # The names of the images you wish to promote. # They need to have been built \u0026amp; pushed by a previous build step. TO_PROMOTE: \u0026#34;\u0026#34; script: - | if [ \u0026#34;${PROXY_TAG}\u0026#34; = \u0026#34;latest\u0026#34; ]; then echo \u0026#34;Nothing to promote.\u0026#34; else echo \u0026#34;Promoting docker image.\u0026#34; docker login -u ${CI_REGISTRY_USER} -p ${CI_REGISTRY_PASSWORD} ${CI_REGISTRY_IMAGE} for IMAGE in ${TO_PROMOTE} ; do docker pull ${CI_REGISTRY_IMAGE}/${IMAGE}:${CI_COMMIT_SHA} docker tag ${CI_REGISTRY_IMAGE}/${IMAGE}:${CI_COMMIT_SHA} ${CI_REGISTRY_IMAGE}/${IMAGE}:latest docker push ${CI_REGISTRY_IMAGE}/${IMAGE}:latest done fi ","permalink":"https://tamir.dev/posts/gitlab-ci-tricks/","summary":"In writing our CI setup at Vdoo, we came across some interesting challenges. Having solved them and used the solutions for quite a while, we decided it is best to share, and hopefully save others some time and effort solving similar problems.\nOf course, there are alternative solutions to the challenges we have dealt with, and some solutions are probably superior to ours. We are happy to hear about such solutions and to improve our own.","title":"GitLab CI Tricks"},{"content":"Last week I had to fix a memory leak in a Python program for the first time. A long running process started eating too much RAM (only ~20GB to much) and the friendly OOM Killer had to step in and terminate this. Since this kept happening, I had to go ahead and fix the issue.\nStep 1 - Reproduction As with every bug, before you can reliably fix it, you must reproduce it.\nNow, while I had a reliable reproduction (after all, the process had regular dates with the OOM Killer), 3 days isn\u0026rsquo;t the best cycle time when you wanna solve a bug. So into the code we go.\nThe main idea is to start with the main loop, and try to narrow down the code that is must run for the leak to manifest. The process involves some educated guesses (where are the likely memory and allocation hogs in your process? What parts are likely to leak? Do you have any code that requires cleanup?), waiting, frustration, and tools.\ntracemalloc While each developer and codebase have their own unique guesses and frustrations, good tooling applies more widely. For this part, I used Python\u0026rsquo;s tracemalloc module.\nAmong other things, tracemalloc allows tracking memory usage between 2 points in your code in a very low-overhead manner.\n1 2 3 4 5 tracemalloc.start() # Start the memory trace code_suspected_of_leak() current, peak = tracemalloc.get_traced_memory() # Get memory stats After running this code, peak will hold the peak-memory-usage during the trace period, and current will hold the difference from the start of the trace to the current state. You should expect current to be non-zero. But if it goes too high - your code is probably leaking.\nBy placing such traces around suspect pieces of our code, we can find which parts are leaking. Just remember - only do this with functions that are expected to retain no state. If a function mutates an external object, or is a member function, it is very to exhibit changes in memory usage.\nStep 2 - Triage Once we have a reproduction (that hopefully takes a relatively short amount of time), we want to find the leaking code. We can try and keep narrowing our measured code down until we find the relevant line, but the deeper we go, the harder it is to separate the leak from normal execution.\nSo at this point, we\u0026rsquo;d like to look into the allocated memory, and see which objects are there when they shouldn\u0026rsquo;t be.\npympler For inspecting the objects in a Python process, I recommend using pympler.\nPympler is a development tool to measure, monitor and analyze the memory behavior of Python objects in a running Python application.\nWe\u0026rsquo;re going to use it to do 2 things.\nInspecting Allocated Objects First, we\u0026rsquo;re going to use pympler to show us which objects were allocated during our repro \u0026amp; are still allocated.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 from pympler import tracker, muppy, refbrowser from functools import lru_cache # Naive code to trigger a leak class Value: def __init__(self, value): self._value = value def __repr__(self): return f\u0026#34;Value({self._value})\u0026#34; @lru_cache(maxsize=100) def get_value(value): return Value(value) def code_suspected_of_leak(): for x in range(10): print(get_value(x)) # Measuring code def main(): tr = tracker.SummaryTracker() code_suspected_of_leak() tr.print_diff() Once we run this, we get a nice table showing us a summary of objects created and destroyed:\n1 2 3 4 5 6 7 8 9 10 11 types | # objects | total size ======================= | =========== | ============ list | 4892 | 500.59 KB str | 4887 | 341.45 KB int | 1053 | 28.79 KB dict | 13 | 1.90 KB __main__.Value | 10 | 640 B function (store_info) | 1 | 144 B cell | 2 | 112 B weakref | 1 | 88 B tuple | -8 | -680 B As you can see - there are quite a few primitive objects generated, and also some __main__.Value objects. In my experience, primitives are harder to track, as they lack meaning in the code. Your own types, however, are usually only used in certain parts of the codebase, making them easier to make sense of.\nNow that we see that we have 10 new Value objects, it is time to figure out who\u0026rsquo;s holding them in memory.\n1 2 3 4 5 6 7 def output_function(o): return str(type(o)) all_objects = muppy.get_objects() root = muppy.filter(all_objects, Value)[0] cb = refbrowser.ConsoleBrowser(root, maxdepth=2, str_func=output_function) cb.print_tree() This\u0026rsquo;ll print the following:\n1 2 3 \u0026lt;class \u0026#39;__main__.Value\u0026#39;\u0026gt;-+-\u0026lt;class \u0026#39;list\u0026#39;\u0026gt; +-\u0026lt;class \u0026#39;functools._lru_cache_wrapper\u0026#39;\u0026gt;-+-\u0026lt;class \u0026#39;list\u0026#39;\u0026gt; +-\u0026lt;class \u0026#39;dict\u0026#39;\u0026gt; Giving away the issue - the lru_cache is keeping our Value objects. Just as designed\u0026hellip;\nI know this looks like a bit of a contrived example, but the lru_cache keeping objects in memory was exactly the issue I had. It was just buried under far more code.\nStep 3 - Solution Currently, I use the ugliest solution I can imagine - functions decorated with lru_cache have a cache_clear() method, and I\u0026rsquo;m calling that at specific places in my code. It\u0026rsquo;s ugly, but it works.\nA cleaner solution would require dedicated caches \u0026amp; better cleanup mechanisms. You can read a relevant discussion here.\n","permalink":"https://tamir.dev/posts/finding-a-memory-leak-in-my-python-code/","summary":"Last week I had to fix a memory leak in a Python program for the first time. A long running process started eating too much RAM (only ~20GB to much) and the friendly OOM Killer had to step in and terminate this. Since this kept happening, I had to go ahead and fix the issue.\nStep 1 - Reproduction As with every bug, before you can reliably fix it, you must reproduce it.","title":"Finding a memory-leak in my Python code"},{"content":"This post is brought to you in the spirit of converting tweetstorms to blogposts. to the tweetstorm\nSurprise! 🎁 In Python, if property access raises AttributeError, and the class implemented getattr, it will get called with the property name. This results in some very cryptic errors.\nIf you run the following code (repl):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Thing: name = \u0026#34;Thing\u0026#34; class NameProvider: def __init__(self, name): self.name = name def get_name(self): return self.nam class ThingWrapper: def __init__(self, thing, name_provider): self.thing = thing self.name_provider = name_provider @property def name(self): return self.name_provider.get_name() def __getattr__(self, name): return getattr(self.thing, name) def main(): thing = Thing() name_provider = NameProvider(name=\u0026#34;Not Thing\u0026#34;) thing_wrapper = ThingWrapper(thing, name_provider) print(thing.name) print(thing_wrapper.name) if __name__ == \u0026#39;__main__\u0026#39;: main() You\u0026rsquo;ll get a surprising result:\n1 2 Thing Thing You might have expected \u0026quot;Not Thing\u0026quot; as the second line, or maybe an exception to be raised from NameProvider.get_name() due to the typo there (self.nam instead of self.name). But instead, we got the name attribute from our Thing instance.\nAnalysis 🔎 If you\u0026rsquo;ve every used __getattr__() you know that it is called when the named attribute was not found using other lookup mechanisms. That said, it might not be clear to you that this includes properties raising AttributeError exceptions. It definitely wasn\u0026rsquo;t clear to me.\nThat is, it was unclear to me despite being clearly stated in the documentation for getattr()\nobject.__getattr__(self, name) Called when the default attribute access fails with an AttributeError (either __getattribute__() raises an AttributeError because name is not an instance attribute or an attribute in the class tree for self; or __get__() of a name property raises AttributeError). This method should either return the (computed) attribute value or raise an AttributeError exception.\nBeside being surprising, there are 2 main issues here:\nAny code down the stack from the property can effectively change attribute lookup for the class by throwing an AttributeError. In the above example - a typo in NameProvider caused an attribute to be taken from Thing instead, against the programmer\u0026rsquo;s obvious intention. The exception is silenced. There is no way for the programmer to catch the exception outside the property getter. This makes the errors very hard to track down. This also means that whenever you add __getattr__() to a class, you\u0026rsquo;re silencing all AttributeError exceptions that were previously thrown from properties. Like anything in Python, you can hack around the issue. In this case - with fancy decorators!\nSolution? 🐍 Consider the following code (repl):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 class ExceptionCatcher: def __init__(self, f): self.f = f self.exception = None def __call__(self, *args, **kwargs): try: return self.f(*args, **kwargs) except Exception as e: self.exception = e raise else: self.exception = None def store_exception(f): return ExceptionCatcher(f) def load_exception(f): def _raise_property_exception(instance, name): try: class_attr = getattr(instance.__class__, name) if not isinstance(class_attr, property): return exception = class_attr.fget.exception except AttributeError: return if exception: raise exception def _wrapper(*args, **kwargs): _raise_property_exception(*args, **kwargs) return f(*args, **kwargs) return _wrapper class ThingWrapper: def __init__(self, thing, name_provider): self.thing = thing self.name_provider = name_provider @property @store_exception def name(self): return self.name_provider.get_name() @load_exception def __getattr__(self, name): return getattr(self.thing, name) If you run this version, you\u0026rsquo;ll get the following exception:\n1 AttributeError: \u0026#39;NameProvider\u0026#39; object has no attribute \u0026#39;nam\u0026#39; This matches our expectations far better.\nThis result is achieved in two steps. First, we store all the exceptions thrown from name() so that we can throw them again if needed. Then, before calling __getattr__(), we check if we got there due to a property raising an exception. If we did - we just re-raise that exception.\nThe rest is implementation details, and I probably missed something there (you might notice that I corrected a bug when converting the tweets to this post - in the previous version, I forget to reset the exception storage after successful property retrieval).\nWhile this solution works, and may be useful for detecting similar bugs, I would probably avoid using it in production code. Instead, I\u0026rsquo;d be happy to have some standard Python construct to provide this functionality.\n","permalink":"https://tamir.dev/posts/til-python-attribute-lookup-order-is-tricky/","summary":"This post is brought to you in the spirit of converting tweetstorms to blogposts. to the tweetstorm\nSurprise! 🎁 In Python, if property access raises AttributeError, and the class implemented getattr, it will get called with the property name. This results in some very cryptic errors.\nIf you run the following code (repl):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class Thing: name = \u0026#34;Thing\u0026#34; class NameProvider: def __init__(self, name): self.","title":"TIL Python attribute lookup order is tricky"},{"content":"Today we set out to implement a feature I saw and liked in Kotlin - Extension Methods.\nYou can follow along with working code samples here, or get the code here\nExtension methods are a nice piece of syntactic-sugar that allow you to define free-functions and call them like instance methods. In Kotlin, it looks something like this:\n1 2 3 4 5 6 7 8 fun Square.draw() { drawSquare(this) } // ... val square = getSquare() square.draw() Now, since they are free, static functions, they follow the same rules. They are not part of the class, nor have access to private members. And they can only be called in a scope where they are visible. Adding them in your code does not affect other code. Additionally, true member functions, if they exist, take precedence over extension methods (this is especially important with generic extension methods).\nIn our code today, we\u0026rsquo;ll try to mimic the features of extension methods as closely as possible. We\u0026rsquo;ll use the following syntax:\n1 2 3 @extend(Square) def draw(square): draw_square(square) For extension methods, and the following implementation of Square in our code throughout:\n1 2 3 4 5 from dataclasses import dataclass @dataclass class Square: length: int Monkey Patching 🙈 Python is a very dynamic language. Among other things, it allows us to change the attributes of (non-builtin) types at run-time. This means that we can extend our Square class by adding a draw method to it at run-time.\n1 Square.draw = draw_square We\u0026rsquo;re now free to call square.draw(). Before we discuss the draw-backs, let\u0026rsquo;s implement it with the syntax we defined:\n1 2 3 4 5 6 7 8 def monkey_extend(cls): def _decorator(f): setattr(cls, f.__name__, f) return _decorator @monkey_extend(Square) def draw(square): draw_square(square) Let\u0026rsquo;s go over this. monkey_extend is a decorator with arguments. This is a common pattern where we use a decorator factory (monkey_extend) to create a new decorator (_decorator) as a closure, giving it access to the parameters passed to the factory (cls). Then, in the core of the decorator, we use setattr to do our monkey-patching.\nWhile this works, it has several issues:\nScope - once set, it can be used with any Square in any scope Precedence - it will override any existing Square.draw Dealing with precedence is easy (using hasattr to check for existing .draw) so we\u0026rsquo;ll focus on the scoping first.\nDynamic Attribute Lookup ✨ The first thing we know is that we need our new attribute to be there in some cases, and be gone in others - we need dynamic resolution. To do that, we\u0026rsquo;ll use __getattr__. In Python classes, __getattr__ is used in attribute lookup as a last resort, called when the other ways of looking up attributes came up empty. We\u0026rsquo;ll write our __getattr__ along the following lines:\n1 2 3 4 5 6 def my_getattr(obj, name): if not has_extension(obj, name): raise AttributeError() if not is_in_scope(name): raise AttributeError() return our_extension The first check, has_extension, is basically checking whether the name we got matches the name of our extension method. Nothing to elaborate yet. Scoping, once again, remains the trickier part.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import functools import inspect from collections import ChainMap def scoped_extend(cls): def _decorator(f): def _getattr(obj, name): # (2) if name != f.__name__: raise AttributeError() # (3) frame = inspect.stack()[1].frame scope = ChainMap(frame.f_locals, frame.f_globals) if scope.get(f.__name__) == f: raise AttributeError() # (4) return functools.partial(f, obj) # (1) cls.__getattr__ = _getattr return f return _decorator This is a bit much, so we\u0026rsquo;ll go over it in detail.\nAs a basis, we used the same decorator-with-parameters pattern here. We have scoped_extend take the class we want to extend, then return _decorator to get the job done. But instead of setting the attribute we want to extend, we monkey-patch cls\u0026rsquo;s __getattr__ to our implementation (See (1)). This will override any existing implementation of __getattr__, but we\u0026rsquo;ll get to that later. For now, we\u0026rsquo;ll focus on our implementation of __getattr__.\nIn (2) we implemented has_extnesion - we simply compare the name we got to the name of our extension method. Then, in (3), comes some Python magic. Python allows us to inspect the running program, to see where we were called from and what variables were in scope in that code. To do that, we use the inspect module. We use inspect.stack() to get the call-stack for the current execution, then access the second frame ([1]) to get our caller. This will be where getattr(obj, name) is invoked or obj.name is used. We use .frame to get the execution frame, and .f_locals and f_globals to get the local and global variables available in that scope. They are equivalent to calling globals() or locals() in the relevant frame.\nWith the scope at hand, we perform a lookup to see whether the extension method we defined is in that scope. To make sure we have our extension method, we get it by name, then ensure that it is truly our method.\nFinally, in (4), when we know our method should be active, we bind it to the instance of the extended class and return it.\nBetter Scoping While our scope retrieval code works, it\u0026rsquo;s better to put it in a function rather than use it inline:\n1 2 3 def _is_in_scope(name, value): frame = inspect.stack()[2].frame return ChainMap(frame.f_locals, frame.f_globals).get(name) == value But, oh, we have to increment the stack index to 2 since we\u0026rsquo;re deeper in the callstack. This is risky. Instead, we\u0026rsquo;ll use the following trick to get the frame:\n1 2 3 4 5 6 7 8 9 def _get_first_external_stack_frame(): for frameinfo in inspect.stack(): if frameinfo.filename == __file__: continue return frameinfo.frame def _is_in_scope(name, value): frame = _get_first_external_stack_frame() return ChainMap(frame.f_locals, frame.f_globals).get(name) == value Instead of counting the frames in our code, changing them with every change - we\u0026rsquo;ll use the module system. We know that all of our scaffolding is in the same module, but the usage is not. This allows us to easily traverse the stack until we find code that does not belong in our module. That is our calling code.\nSince you\u0026rsquo;re probably wondering - yes. You need to change _get_first_external_stack_frame() if you want to put it in a different module. Implementing it is left as an exercise to the reader.\nPreserving __getattr__ As mentioned before, our current implementation overrides any existing __getattr__ function for the class. Lucky for us, fixing it is easy:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 def no_override_extend(cls): def _decorator(f): def _default(_obj, _name): raise AttributeError() original_getattr = getattr(cls, \u0026#39;__getattr__\u0026#39;, _default) def _getattr(obj, name): with suppress(AttributeError): return original_getattr(obj, name) if name != f.__name__: raise AttributeError() if not _is_in_scope(f): raise AttributeError() return functools.partial(f, obj) cls.__getattr__ = _getattr return f return _decorator In (1) we get the original __getattr__ method, to be stored for later usage. We use the _default function to avoid an extra if later. In (2) we use the saved __getattr__, making sure that we only proceed to our code if it raised an AttributeError exception.\nInterlude 🐍 With no_override_extend we have our first \u0026ldquo;to-spec\u0026rdquo; implementation of extension methods. We have both scoping and precedence down. It is time to celebrate and rest. But our quest is not done yet.\nWhile our code works well for a proof-of-concept, there are still significant usability issues with it. Since the extension methods we create have nice and clean names, it is likely that we\u0026rsquo;ll want to use those names for other things. Unfortunately, once we do that, we\u0026rsquo;ll override the existing extension methods and they will no longer work:\n1 2 3 4 5 6 7 8 9 10 @extend(Square) def draw(square): draw_square(square) def draw(): print(\u0026#34;Drawing is awesome!\u0026#34;) # ... square.draw() # This will fail, as `draw` has been replaced in this scope. Indirection 🔀 The Fundemental Theorem of Software Engineering (FTSE) says that any problem can be solved by adding another level of indirection. Let\u0026rsquo;s see how this applies to our problem.\nAs mentioned in the interlude, our main issue is that of naming. Our extension method is bound to a name, and that name can be overriden in the scope that defines it. If that happens, we lose our extension method. To solve that, we\u0026rsquo;ll add another level of indirection - a scope that can safely hold our extension methods and protect them from being overriden. If you read our previous post you might recall that classes are wonderful for scopes. So we\u0026rsquo;ll use a class.\nOur new syntax will look like this:\n1 2 3 4 @extension class ExtensionMethods(Square): def draw(self): draw_square(self) While we\u0026rsquo;re still using a decorator, you may notice that it takes no parameters. Instead, we use the extended type as the base type for our extension class. This allows us to write the extensions like any other subclass, with standard Python syntax, and then use the decorator to install the extensions in it.\nSince we\u0026rsquo;ve already gone over the principles behind the construction of the decorator, let\u0026rsquo;s jump straight to the code and focus on the differences from the previous version:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 def extension(scope_cls): def _default(_obj, _name): raise AttributeError() # (1) cls = scope_cls.__base__ original_getattr = getattr(cls, \u0026#39;__getattr__\u0026#39;, _default) def _getattr(obj, name): with suppress(AttributeError): return original_getattr(obj, name) # (2) if not hasattr(scope_cls, name): raise AttributeError() # (3) if not _is_in_scope(scope_cls): raise AttributeError() # (4) f = getattr(scope_cls, name) return functools.partial(f, obj) cls.__getattr__ = _getattr return scope_cls First, you can see that there is no nested decorator - only the main one. And, as we mentioned before, we use inheritance to indicate which type we\u0026rsquo;re extending. So in (1) we access the base-class of our extension class to get the class we\u0026rsquo;re extending. Then, in (2) we check whether the requested attribute exists in our extension class. As you can see, the changes are pretty simple and straight-forward. In (3) we make the most important change - we check for the extension class in the scope, not the extension methods. This is the core of this change! And lastly, in (4), we get the required attribute from out extension class.\nAnd with that, we\u0026rsquo;re done.\nFinal Words I hope you enjoyed this article. Regardless of that, I hope you never use it in production code.\n","permalink":"https://tamir.dev/posts/snake-eyes-extension-methods/","summary":"Today we set out to implement a feature I saw and liked in Kotlin - Extension Methods.\nYou can follow along with working code samples here, or get the code here\nExtension methods are a nice piece of syntactic-sugar that allow you to define free-functions and call them like instance methods. In Kotlin, it looks something like this:\n1 2 3 4 5 6 7 8 fun Square.draw() { drawSquare(this) } // .","title":"Snake Eyes: Extension Methods"},{"content":"One of my pet peeves is taking concepts from other languages and \u0026ldquo;translating\u0026rdquo; them to Python. Not because it makes good code, but because it\u0026rsquo;s a challenge and it makes me happy.\nThis time, I\u0026rsquo;ve gone after two simple concepts - nested code blocks and IIFE. Both serve similar purposes, and both are missing from Python.\nIn C++, blocks are often used to limits the lifetime of objects and keep them out of our way when we\u0026rsquo;re done with them. In Python, lifetime is usually less of a concern (as we replace RAII and destructors with context-managers), but having variable names out of our way is desirable.\nIIFE offers us a bit more in terms of functionality, as it both creates a scope for our operations, and allows us to get a value from that scope. This is useful both for simpler flow control, and for easily initializing const-qualified variables.\nPython does not have any of those constructs. There is no way to create a nested code-block in Python (adding another level of indentation would just have it complain about unexpected-indent), and while lambdas exist, they only allow for a single expression, making them mostly irrelevant for IIFE. On the other hand, Python offers us two wonderful constructs that can be used virtually everywhere - classes and functions.\nClasses \u0026amp; Nested Blocks 🧱🧱🧱 In Python, both classes and functions can be nested. You can define a class inside a class, a function in a function, a class in a function or a function in a class. It is all the same. What\u0026rsquo;s more - you can have flow-control in both function (well, obviously) and class bodies (meta-programming much?). Additionally, both nested functions and nested classes create new variable scopes, keeping their insides inside, and are also closures, capable of capturing values from their enclosing scopes. As such, they are the perfect tools for our language-bending shenanigans.\nFirst, nested code blocks. I offer you the following solution:\n1 2 3 4 5 6 7 8 9 10 def f(x): print(\u0026#39;Classes are great for creating blocks.\u0026#39;) class _: y = x * 2 print(f\u0026#39;y = {y}\u0026#39;) print(\u0026#39;y is not defined here.\u0026#39;) y f(21) By defining a class, we create a new scope. Inside it, we can do whatever we want, knowing that the code will get execute inline and in order, and the results will not leak into the enclosing scope.\nThat said - there are some caveats. First, the class remains in scope, and so do all the variables defined in it. They cannot be garbage collected until the function terminates. You can verify it yourself by trying to access _.y in the above example. To remedy that, we need to get rid of the class, or at least its contents. There are many ways to achieve it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # Replace the class with a bool @bool class _: x = 1 print(x) # Replace the class with None def empty(*args): return None @empty class _: x = 1 print(x) # Use a metaclass to delete all the variables inside the class class BlockMeta(type): def __new__(cls, name, bases, dict_): return super().__new__(cls, name, bases, {}) class Block(metaclass=BlockMeta): pass class _(Block): x = 1 print(x) I am personally torn between the meta-class approach, as it is explicit and clear, and the @bool approach, as it requires to additional boilerplate.\nThe second issue with classes as blocks is that while they can be nested freely, a nested class cannot access the variables of its nesting class, rendering block-nesting moot. I do not have a solution for that at present.\nFunctions \u0026amp; IIFE 🐍🐍🐍 With a solution for nested blocks in hand, it is time to get proper IIFE in Python. For that, we\u0026rsquo;ll naturally be using function. Along with those, we\u0026rsquo;ll use a function\u0026rsquo;s best friend - the decorator!\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def iife(f): return f() def describe_number(n): @iife def message(): if n \u0026lt; 0: return f\u0026#39;{n} is smaller than 0\u0026#39; elif n \u0026gt; 0: return f\u0026#39;{n} is larger than 0\u0026#39; return f\u0026#39;{n} is 0\u0026#39; print(message) describe_number(-1) describe_number(0) describe_number(1) Using the decorator, we immediately call the function, binding the function\u0026rsquo;s name to the return value instead of the function itself. A function returning None (or without a return statement) will just bind the name to None.\nWhile this looks a bit more messy, it can also double as a solution for nested blocks. And unlike the class solution - it can be freely nested.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def iife(f): return f() def block(f): f() def f(x): print(\u0026#39;Functions are great for creating blocks.\u0026#39;) @block def _(): my_x = x + 1 @iife def y(): return my_x * 2 @block def _(): print(f\u0026#39;y = {y}\u0026#39;) print(\u0026#39;y is not defined here.\u0026#39;) y f(20) That\u0026rsquo;s it for today. I hope you had some fun.\n","permalink":"https://tamir.dev/posts/snake-eyes-scopes-and-iife/","summary":"One of my pet peeves is taking concepts from other languages and \u0026ldquo;translating\u0026rdquo; them to Python. Not because it makes good code, but because it\u0026rsquo;s a challenge and it makes me happy.\nThis time, I\u0026rsquo;ve gone after two simple concepts - nested code blocks and IIFE. Both serve similar purposes, and both are missing from Python.\nIn C++, blocks are often used to limits the lifetime of objects and keep them out of our way when we\u0026rsquo;re done with them.","title":"Snake Eyes: Scopes and IIFE"},{"content":"After multiple attempts at finding a funny narrative that holds for the entire article and failing miserably, I decided to go with the technical parts alone. Enough of my colleagues found it interesting, so I guess it will hold up without the jokes.\nPython gives us multiple ways to check that the objects we pass to functions are of the types we expect. Each method has its advantages and disadvantages.\nJust not caring The first option is naturally to not care about types - just write your code, and hope for the best. This is a viable method, and is often employed. It is especially fitting in short snippets or scripts you don\u0026rsquo;t expect to maintain much. It just works with no overhead whatsoever.\n1 2 3 def poke(duck): duck.quack() duck.walk() Inheritance Another option, as common in OOP languages, is to use inheritance. We can define an Anas class, and expect all of its derivatives to be sufficiently duck-like.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Anas: def quack(self): pass def walk(self): pass class Duck(Anas): def quack(self): print(\u0026#39;Quack!\u0026#39;) def walk(self): print(\u0026#39;Walks like duck.\u0026#39;) class Mallard(Anas): def quack(self): print(\u0026#39;Quack!\u0026#39;) def walk(self): print(\u0026#39;Walks like duck.\u0026#39;) def poke(duck): assert isinstance(duck, Anas) Interfaces While inheritance kinda gets the job done, a robotic duck is definitely not of the genus Anas, while it does implement all the characteristics we care about. So instead of hierarchical inheritance, we can use interfaces.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from abc import ABC, abstractmethod class IDuck(ABC): @abstractmethod def quack(self): pass @abstractmethod def walk(self): pass class Duck(IDuck): def quack(self): print(\u0026#39;Quack!\u0026#39;) def walk(self): print(\u0026#39;Walks like duck.\u0026#39;) class RoboticDuck(IDuck): def quack(self): print(\u0026#39;Quack!\u0026#39;) def walk(self): print(\u0026#39;Walks like duck.\u0026#39;) def poke(duck): assert isinstance(duck, IDuck) Great. And if we don\u0026rsquo;t control the types, we can always write adapters.\nThe Duck Test But this is Python. We can do better.\nAs we know, Python uses duck-typing. So we should be able to use the Duck Test for types. In our example, every object implementing quack() and walk() is a duck. That\u0026rsquo;s easy enough to check.\n1 2 3 4 5 6 7 8 9 10 def is_a_duck(duck): for attr in (\u0026#39;quack\u0026#39;, \u0026#39;walk\u0026#39;): if not hasattr(duck, attr): return False return True def poke(duck): assert is_a_duck(duck) duck.quack() duck.walk() This works. But we list the isinstance(...) call. We can surely do better.\nMetaclasses \u0026amp; Subclass Hooks Metaclasses are wonderful constructs. As their name may suggest, they take part in the construction of classes. They even allow us to set hooks into basic Python mechanisms, like isinstance(...), using __subclasshook__.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from abc import ABC def is_a_duck(duck): for attr in (\u0026#39;quack\u0026#39;, \u0026#39;walk\u0026#39;): if not hasattr(duck, attr): return False return True class DuckChecker(ABC): @classmethod def __subclasshook__(cls, C): if cls is not DuckChecker: return NotImplemented return is_a_duck(C) def poke(duck): assert isinstance(duck, DuckChecker) duck.quack() duck.walk() And we\u0026rsquo;re back in business. That said, is_a_duck is still a stringly-typed mess, and gonna be very painful to maintain.\nWouldn\u0026rsquo;t it be nice if we could just use our IDuck interface to check for duck-ness?\nAbstract Methods, Again! Lucky for us - we can!\nAmong other things, the ABC parent class enumerates all @abstractmethods and stores them in the __abstractmethods__ member variable. This means that we can easily enumerate them in our subclass hook and check for them.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from abc import ABC, abstractmethod class IDuck(ABC): @abstractmethod def quack(self): pass @abstractmethod def walk(self): pass @classmethod def __subclasshook__(cls, C): if cls is not IDuck: return NotImplemented for attr in cls.__abstractmethods__: if not hasattr(C, attr): return False return True class Duck: def quack(self): print(\u0026#39;Quack!\u0026#39;) def walk(self): print(\u0026#39;Walks like a duck.\u0026#39;) def poke(duck): assert isinstance(duck, IDuck) duck.quack() duck.walk() poke(Duck()) # Quack! # Walks like a duck. Awesome. Next step - separating the interface from the checking logic.\nProtocols Reading through Python documentation and nomenclature, you might have seen the term \u0026ldquo;protocol\u0026rdquo; here and there. It is Python\u0026rsquo;s way to call duck-typed interfaces. So you could say we just created a \u0026ldquo;protocol checker\u0026rdquo;. Now, we can separate it into a base-class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from abc import ABC, abstractmethod class Protocol(ABC): _is_protocol = True @classmethod def __subclasshook__(cls, C): if not cls._is_protocol: return NotImplemented for attr in cls.__abstractmethods__: if not hasattr(C, attr): return False return True class IDuck(Protocol): @abstractmethod def quack(self): pass @abstractmethod def walk(self): pass And that\u0026rsquo;s it. That little _is_protocol flag is there for good reason. Usually, we\u0026rsquo;d check protocol-ness using isinstance(...). In this case, however, we\u0026rsquo;re hooking into that mechanism and that would lead to infinite recursion.\nWe can now use our Protocol base-class freely to create new protocols as we need them, with friendly interface-like syntax. We\u0026rsquo;re almost done.\nThis Dog is a Duck In some cases, the protocol checks may not be what we want. The obvious reasons coming to mind are:\nWe can\u0026rsquo;t really check the desired semantics using the protocol trick. We want to wreck havoc. For those cases (well, mostly for the first one) the ABC base class provides another trick. Instead of defining __subclasshook__ to check the interface, we can simple register classes as valid, \u0026ldquo;virtual subclasses\u0026rdquo;.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from abc import ABC class IDuck(ABC): pass class Duck: def quack(self): print(\u0026#39;Quack!\u0026#39;) def walk(self): print(\u0026#39;Walk like a duck.\u0026#39;) IDuck.register(Duck) def poke(duck): assert isinstance(duck, IDuck) duck.quack() duck.walk() Remember that this method puts all the pressure on the programmer. Writing IDuck.register(Dog) is the equivalent of saying \u0026ldquo;I vouch for this dog to be a duck\u0026rdquo;. It might pass inspection, but won\u0026rsquo;t necessarily yield the desired results.\nSummary In this article we covered multiple ways of checking the \u0026ldquo;duck-ness\u0026rdquo; of objects. From belonging to the Anas genus, to just placing a sticker on their head saying \u0026ldquo;Duck!\u0026rdquo;. Some of those methods are more useful or applicable than others, but I still think it worthwhile to be familiar with all of them. Additionally, there are many topics not covered here, like static type checking.\nFurther Reading The metaclass techniques demonstrated here are simplified versions of code from the abc and typing modules. I highly recommend going through those modules and their respective docs, at least at a glance, to extend your knowledge and cover up any holes left by my hand-wavy explanation.\n","permalink":"https://tamir.dev/posts/recognizing-ducks/","summary":"After multiple attempts at finding a funny narrative that holds for the entire article and failing miserably, I decided to go with the technical parts alone. Enough of my colleagues found it interesting, so I guess it will hold up without the jokes.\nPython gives us multiple ways to check that the objects we pass to functions are of the types we expect. Each method has its advantages and disadvantages.","title":"Recognizing Ducks"},{"content":"Internationalization is a difficult problem is software-engineering. Usually that statement would refer to the technical aspects of providing a good user experience for your customers. Today, however, I am referring to the social aspects.\nJust as it has become common-place to have your users spread across the globe, so it has with developers. It is not uncommon to work with teams in another country, or even to have a specific team-member working from a remote location. Being able to cooperate with developers across the globe is a great enabler. But as the case is with - well - everything, it has some obvious - and some less obvious - pitfalls to avoid.\nWhere (When?) Are My Colleagues? First and foremost - know the time-zones your colleagues reside in.\nYou already know that some of your teammates tend to arrive a bit early or a bit late. You know it, and you adjust to it. You won\u0026rsquo;t make a colleague who starts 2 hours before you stay for a meeting when you know they should be picking up their kids from kindergarten, right?\nWell, all you need to do now is adjust to having teammates arriving at work 10 hours before you.\nTime For Your 12AM Daily Standup Yes, that\u0026rsquo;s AM. The one that\u0026rsquo;s confusingly set at midnight.\nNow imagine that you\u0026rsquo;re in a UTC-07:00 time-zone (Pacific Daylight Time) and the rest of your team is in a UTC+03:00 time-zone (Israel Daylight Time). Your daily-standup is at 10AM, which sounds reasonable. But that\u0026rsquo;s in Israel. Sadly, this translates to 12AM for you. But you can accommodate that, can\u0026rsquo;t you?\nLuckily, in the real-world company HQs tend to be in the USA, not Israel. So you\u0026rsquo;re safe. You can hold your meetings at any time you want, and they\u0026rsquo;ll have to adjust. You can have them wake up early, or stay up late, or just work very awkward shifts. They\u0026rsquo;ll accommodate. But do you really want to make your teammates do that?\nGet It Done By EOD Today No.\nIt\u0026rsquo;s not that I don\u0026rsquo;t want to, it simply isn\u0026rsquo;t possible.\nIf you have a large enough time-difference (say, 6 hours difference over a 9 hours work-day) your day starts when your colleagues\u0026rsquo; day ends (or later!)\nNo matter how hard they work, they cannot get it done by EOD if EOD has already passed. You can talk about getting things done \u0026ldquo;by tomorrow\u0026rdquo;, but not EOD. It causes un-needed tension as the need to explain this comes up over and over during planning meetings.\nPlanned Downtime: 9PM-6AM Great. You\u0026rsquo;ve just killed of a day\u0026rsquo;s work for your distant colleagues. 9PM San-Francisco is 7AM in Tel-Aviv. Making it a 7AM-4PM downtime for them.\nI\u0026rsquo;m not saying you can\u0026rsquo;t do that, but you should try and keep that in mind.\nAnd Then, There\u0026rsquo;s Israel And on the seventh day God ended His work which He had done, and He rested on the seventh day from all His work which He had done.\nThen God blessed the seventh day and sanctified it, because in it He rested from all His work which God had created and made.\nGenesis 2:2-3\nAnd so in Israel we rest on the 7th day, and add the 6th in for good measure.\nWe work Sunday through Thursday, and rest on Friday and Saturday. And this is annoying.\nIt is annoying when we need support on Sunday and have to wait.\nIt is annoying when we travel, and are never sure if we should report Friday as overtime, or Sunday as a day off.\nAnd it is annoying to be expected to work on Friday evening just because it\u0026rsquo;s a working-day for you. You wouldn\u0026rsquo;t want to work Saturday nights, would you? I didn\u0026rsquo;t think so.\nAlso, you should expect email send on Fridays and late Thursdays to be ignored until Sunday.\nIn Conclusion This has been a bit of a rant, and a bit of advice as well. I\u0026rsquo;m not sure which dominated the tone.\nThis might seem like a minor issue, but it can cause a great deal of pain when ignored.\nBut in the end, the rules are simple. All you need to do is be aware, and be considerate. You can do it for the people you physically see everyday, and you can do it for your remote colleagues.\nTry and set your long-distance meeting at times suitable for both ends. And when that isn\u0026rsquo;t possible - make sure you make an effort as well. Waking up an hour earlier every now and then goes a long way.\n","permalink":"https://tamir.dev/posts/working-across-timezones/","summary":"Internationalization is a difficult problem is software-engineering. Usually that statement would refer to the technical aspects of providing a good user experience for your customers. Today, however, I am referring to the social aspects.\nJust as it has become common-place to have your users spread across the globe, so it has with developers. It is not uncommon to work with teams in another country, or even to have a specific team-member working from a remote location.","title":"Working Across Time-Zones"},{"content":"In the few years since Rust came out, I\u0026rsquo;ve frequently found myself explaining what an amazing language Rust is. Laying out the ways in which it is going to change the systems-programming world, and what a pleasure Rust is to code in.\nA few months back, I finally got around to trying it out. It was horrid. The darned borrow-checker just wouldn\u0026rsquo;t let me do anything. I couldn\u0026rsquo;t get anything meaningful to compile. Despite years of programming in various languages I felt completely incompetent to handle it. I resolved to drop Rust and focus on other things.\nDespite my initial disappointment and discouragement, I still could not get Rust out of my head. Its premise is too promising to put down after one go. Besides, I cannot be beaten by a programming language! And yet, it can wait. It is too much work for now. Some day, maybe in a few years, I\u0026rsquo;ll write Rust.\nUsing Rust, however, is a completely different experience. Rust tools are absolutely amazing. As a Windows user through-and-through I got used to open-source tools (especially command line tools) not being supported on Windows. (No, cygwin is not a valid solution.) I don\u0026rsquo;t blame the devs - they work on Linux. Even if they have the time to spend on the Windows port, they don\u0026rsquo;t necessarily have a Windows machine to test it on. And yet - I am used to being disappointed. That is why, when I first heard of rg(an amazing grep replacement) and fd(an amazing find replacement) I knew that they would not work on Windows. But, being my optimistic self - I checked. And a good thing I did that.\nTo install Rust tools, the easiest way is to install the Rust toolset and compile them. A daunting task in every other language, yet a breeze in Rust.\nHead over to rustup.rs and install Rust (A single command-line on Linux, a single executable on Windows) cargo install ripgrep fd-find That\u0026rsquo;s it. Really. Now use the tools. This was when I realized how amazing Rust really is. Even if you ignore the language completely - it\u0026rsquo;s tooling and package management is unparalleled. Having published Python packages in the past, I was amazed at the simple publishing and installation mechanisms. Having used C and C++ I was simply amazed at a systems-programming with package management. So while still somewhat scared of the borrow-checker, I decided that my next CLI tool will be written in Rust. The easy-as-pie deployment bought be over completely.\nSome months after that, I finally found myself in need of a new CLI tool. I was faced with a continuous log I wanted to filter for duplicates. sort -u sorts, so it cannot work on streams. Of course, there is probably some sed or awk magic I can use, but I want something simple. Besides, a tool that filters out duplicates seems like the perfect beginner project for getting hands-on with Rust. So I went ahead and creates uq. After finishing it, I published it on crates.io. cargo install uq on a second machine, and it worked. Both Windows and Linux. A friend tried it, and it simply worked! I never had such a pleasant deployment experience. It is practically easier to install from source then send a compiled binary! And it works cross-platform out of the box.\nA short while later I wanted to group some log entries by a regex. I looked around and could not find a simple way to do it. So, once again, I turned to Rust. Some borrow-checker-wrestling later and the first version of groupby was complete. Yay!\nA short time later I had one of the best open-source experiences I\u0026rsquo;ve ever had. Someone started using groupby, looked at my terrible code, and posted this issue:\nHello\nI find this little program will be useful for many things I do (I usually do something like that with combination of sed, sort, …). I also looked into the code. Do I guess right that you\u0026rsquo;re still learning Rust? Could I provide few little tips?\nI glimpsed at least one unwrap that can be triggered by the user input (giving a too large group ID), which will result in ugly error message instead of nice useful one. Do you choose BTreeMap/BTreeSet for some specific reason? Is it the order of elements? If not, HashMap and HashSet are likely to be faster. Both variants (unique vs all) look very similar and differ only in the inner data type and the method used to push/insert. I think this could be done with just one piece of code that is generic over the type, and adding your own trait that implements the adding for either. Would you like me to show you such code?\nHaving someone more experienced in Rust come in and help me improve my very naïve code was great. And it was my first time getting a \u0026ldquo;this is great, may I help you?\u0026rdquo; comment and not a \u0026ldquo;this is great, I want this as well\u0026rdquo; one.\nFor the time being, I keep spending more time wrestling the borrow-checker than writing actual code in Rust. But I am (almost) sure it is due to lack of experience. On the plus-side, I\u0026rsquo;m becoming better at detecting lifetime issues in other languages as well.\nSo, for anyone who hasn\u0026rsquo;t done it yet, I highly recommend using Rust-based tools. Just for the amazing experience of things working (and compiling!) out of the box. Later, if you choose to actually code in it, be sure to brace yourself for a somewhat bumpy ride. Friends tell me that after a time Rust becomes easier, speeding up their development. I\u0026rsquo;m not there yet, but I\u0026rsquo;m working on it.\n","permalink":"https://tamir.dev/posts/adventures-in-rust/","summary":"In the few years since Rust came out, I\u0026rsquo;ve frequently found myself explaining what an amazing language Rust is. Laying out the ways in which it is going to change the systems-programming world, and what a pleasure Rust is to code in.\nA few months back, I finally got around to trying it out. It was horrid. The darned borrow-checker just wouldn\u0026rsquo;t let me do anything. I couldn\u0026rsquo;t get anything meaningful to compile.","title":"Adventures in Rust"},{"content":"Recently I\u0026rsquo;ve been helping \u0026amp; tutoring some true code beginners. Not someone new to a language, but completely new to programming.\nI\u0026rsquo;ve done a lot of training in the past. Both beginner and advanced training, both programming and reverse-engineering. But as green as my previous students have been, they have always had some prior knowledge, some experience with code. In at least one programming language.\nUsually the training is about teaching language features, special tricks, best practices, and getting the trainees familiar with the new patterns. The trainees send out probes, looking for familiar things, and I just fill them in at the right time. They know what they are looking for, or can be easily guided towards the right thing. When people are completely green, they don\u0026rsquo;t.\nThis is a very new experience for me, and it got me thinking a lot about programming and the ways we approach code. The patterns we seek to find or form. The amazing number of things that we do without even thinking as experience programmers. Each of those, no matter how simple, needs to be broken up and explained to new-comers. They have no previous knowledge to build upon for this.\nFrom my current experience, it seems the understanding the meaning of syntax, and understanding forward-flowing programs is easy enough. Conditionals are a non-issue. The first road-block comes with loops. Especially writing loops. Where do I put the return statement? Where do I define my variables? Trying to explain those things, and give simple rules, I came to some useful realizations of useful patterns, and some painful truths about our use of jargon.\nLet\u0026rsquo;s go ahead and see the patterns.\nFind Loops 1 2 3 4 5 6 7 8 9 public static int indexOf(String[] haystack, String needle) { for (int i = 0; i \u0026lt; haystack.length; ++i) { if (needle.equals(haystack[i])) { return i; } } return -1; } In those loops we iterate over the array, looking for an item that fulfills our condition. Once we find it, we immediately return that value. There is no need to declare any variables.\nCount Loops 1 2 3 4 5 6 7 8 9 10 public static int countOf(String[] haystack, String needle) { int count = 0; for (int i = 0; i \u0026lt; haystack.length; ++i) { if (needle.equals(haystack[i])) { count++; } } return count; } In those loops we iterate over the array, looking for items that fulfill our condition. Whenever we find one, we increment the count. Once we exhaust the iteration, we return the counter. The counter and return statement are outside the loop.\nAction Loops 1 2 3 4 5 public static void printAll(String[] haystack) { for (int i = 0; i \u0026lt; haystack.length; ++i) { System.out.println(haystack[i]); } } In those loops we iterate over the array, and perform an action on each and every item. There are no variables and no return statements.\nNow, those simple loops can do quite a lot, and can be expanded and composed to do more. And I find that they help beginners. But did you spot my error? I used the word \u0026ldquo;iterate\u0026rdquo;.\nVocabulary While the meaning of \u0026ldquo;iterate\u0026rdquo; is clear to existing programmers, and looking at the loops you can easily tell that we are iterating or looping over haystack, it is not clear for beginners. Moreover, the words themselves sound weird. This is critical, and becomes more pronounced as you try and loop in slightly more advanced ways\n1 2 3 4 int i = 0; for (Node current = myList.head; current != null; current = current.getNext(), ++i) { // ... } Here, we are looping (or iterating) over myList, but we don\u0026rsquo;t change anything about it, or even access it directly. We do change i (which is no longer our counter) and current which is a node. This makes the code and language quite dissonant. \u0026ldquo;We iterate over myList while maintaining an index\u0026rdquo; is a true statement, but not an immediate translation from the code. The language forces to go in a very roundabout manner. This is true for many languages. But now, consider slightly more modern syntax:\n1 2 for i, node in enumerate(myList): # ... 1 2 3 for i, node := range myList { // ... } 1 2 3 for (i, node) in myList.enumerate() { // ... } In all of those, the situation is far clearer. We can see that we\u0026rsquo;re looping over myList, and it is clear that we have both a node and an index. While this difference might be minor for experienced programmers, it is a world of difference for newcomers.\nLearning to code is not just learning a programming language. Not just learning to think in a specific way. It is learning your own language again. You know English? Well, now you need to learn programming-English. You know Hebrew? Learn programming-Hebrew. We keep changing the meaning of existing words, and expect people to follow and understand them. It is hard. The least we can do is try and minimize the difference between the code we read (programming languages - Java, C, C++, Python, Go, Rust\u0026hellip;) and the code we speak (well, I guess English is a programming language as well?).\n","permalink":"https://tamir.dev/posts/types-of-loops/","summary":"Recently I\u0026rsquo;ve been helping \u0026amp; tutoring some true code beginners. Not someone new to a language, but completely new to programming.\nI\u0026rsquo;ve done a lot of training in the past. Both beginner and advanced training, both programming and reverse-engineering. But as green as my previous students have been, they have always had some prior knowledge, some experience with code. In at least one programming language.\nUsually the training is about teaching language features, special tricks, best practices, and getting the trainees familiar with the new patterns.","title":"Types of Loops"},{"content":"First, an apology. The first part of this post was published on May 26. It is now September. I had most of the code for this part done by then. But finalizing the code took some more effort. Once that was done, explaining took a while. There were quite a few things I had to learn myself first. So now, months later, I present this humble offering to the Gods of C++ and template meta-programming.\nGeneralizing In Part 1 we created our State or SelfReturning class (provided below for reference). It works, but as you can see - required modifications whenever we change the function arguments or return types.\nCompilation\n1 2 3 4 5 6 7 8 9 10 11 struct SelfReturning { using RetType = std::pair\u0026lt;SelfReturning, const Context\u0026gt;; using FuncType = RetType(*)(const Context\u0026amp;, Event); SelfReturning(FuncType func) : _func{func} {}; RetType operator()(const Context\u0026amp; ctx, Event evt) { return _func(ctx, evt); } FuncType _func; }; using State = SelfReturning; The first thing we want to do is get rid of this requirement. First, function arguments. Compilation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 template \u0026lt;class... Args\u0026gt; struct SelfReturning { using RetType = std::pair\u0026lt;SelfReturning, const Context\u0026gt;; using FuncType = RetType(*)(Args... args); // (1) SelfReturning(FuncType func) : _func{func} {}; RetType operator()(Args... args) { // (2) return _func(std::forward\u0026lt;Args\u0026gt;(args)...); } FuncType _func; }; using State = SelfReturning\u0026lt;const Context\u0026amp;, Event\u0026gt;; Here we use variadic templates and perfect forwarding to forward all the function arguments directly to the target function. You can see that in (1) and (2) we use Args... and not the common Args\u0026amp;\u0026amp;.... This is because the types are defined by the class template and are not deduced on the function call.\nWith this behind us, we address the return type. Here we come to another recursive issue. While the return type std::pair\u0026lt;SelfReturning, const Context\u0026gt; depends on our SelfReturning type, SelfReturning itself depends on the return type. This means that just passing in the return type will not work (much like our original return-type issue). To solve it, we use a template-template parameter.\nCompilation\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 template \u0026lt;template \u0026lt;class T\u0026gt; class Base, class... Args\u0026gt; // (1) struct SelfReturning { using RetType = Base\u0026lt;SelfReturning\u0026gt;; //(2) using FuncType = RetType(*)(Args... args); SelfReturning(FuncType func) : _func{ func } {} RetType operator() (Args... args) { return _func(std::forward\u0026lt;Args\u0026gt;(args)...); } FuncType _func; }; template \u0026lt;class T\u0026gt; using PairWithCtx = std::pair\u0026lt;T, const Context\u0026gt;; // (3) using State = SelfReturning\u0026lt;PairWithCtx, const Context\u0026amp;, Event\u0026gt;; In (1), we pass in the template for the return type. In (2), we instantiate the type with our SelfReturning class. As we\u0026rsquo;ve seen before, C++ allows this type of recursion, so we\u0026rsquo;re safe. In (3) we define our return-type template to be a pair with a const Context as the second member. Done.\nBut what if we want to only return the SelfReturning class? For that, we define a new template - an identity template.\n1 2 3 4 5 6 7 template \u0026lt;class T\u0026gt; struct identity { using type = T; }; template \u0026lt;class T\u0026gt; using identity_t = typename identity\u0026lt;T\u0026gt;::type; We define the identity struct to hold a type, and use the identity_t alias to access the type directly. This looks a bit odd, but C++ does not allow us to alias the template parameter directly. When isntatiating the identity_t template with a type, we get the safe type again. Using that, we can return SelfReturning directly.\n1 using State = SelfReturning\u0026lt;identity_t\u0026gt;; Personally, though, I hate having to write down the trivial cases explicitly. So let\u0026rsquo;s use some dirty tricks.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 template \u0026lt;template \u0026lt;class T\u0026gt; class Base = identity_t, class... Args\u0026gt; // (1) struct SelfReturning { using RetType = Base\u0026lt;SelfReturning\u0026gt;; using FuncType = RetType(*)(Args... args); SelfReturning(FuncType func) : _func{ func } {} RetType operator() (Args... args) { return _func(std::forward\u0026lt;Args\u0026gt;(args)...); } FuncType _func; template \u0026lt;class... AltArgs\u0026gt; using WithArgs = SelfReturning\u0026lt;Base, AltArgs...\u0026gt;; // (2) }; In (1) we simply add identity_t as the default argument for Base. This lets us write the most trivial case (return SelfReturning, take no arguments) as SelfReturning\u0026lt;\u0026gt;. Nice. However, if we put anything into this set of template arguments, it will override identity_t. That\u0026rsquo;s what the code at (2) is for. We set WithArgs to be SelfReturning with whatever Base parameter it already has, thus only accepting template parameters for the arguments. Now we can write all of the following with ease. Compilation\n1 2 3 4 5 6 7 using Trivial = SelfReturning\u0026lt;\u0026gt;; using InPair = SelfReturning\u0026lt;PairWithCtx\u0026gt;; using TrivialWithArgs = SelfReturning\u0026lt;\u0026gt;::WithArgs\u0026lt;Context\u0026amp;, Event\u0026gt;; using InPairWithArgs = SelfReturning\u0026lt;PairWithCtx\u0026gt;::WithArgs\u0026lt;const Context\u0026amp;, Event\u0026gt;; // Or alternatively using InPairWithArgs2 = SelfReturning\u0026lt;PairWithCtx, const Context\u0026amp;, Event\u0026gt;; In Part 1 I promised generalizing the SelfReturning class and getting some compile time guarantees. We\u0026rsquo;ve accomplished our generalization goal, so it\u0026rsquo;s time to get some safety in place.\nIncreasing Safety While our use of the switch statement to discern different events is nice and concise, it is also somewhat error prone. It is easy to miss a case (though that can be prevented using compiler errors) or accidentally mistake one event for another. The latter is especially true if we want to pass information along with our event notification. One easy way to avoid those mistakes is to resolve the choice using function overloading instead of switch statements. Consider the following\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #include \u0026lt;cstdio\u0026gt; // (1) enum class EventType {A, B}; void Switch(EventType evt) { switch(evt) { case EventType::A: puts(\u0026#34;A\u0026#34;); return; case EventType::B: puts(\u0026#34;B\u0026#34;); return; } } // (2) struct EventA {}; struct EventB {}; void Overload(EventA) { puts(\u0026#34;A\u0026#34;); } void Overload(EventB) { puts(\u0026#34;B\u0026#34;); } In (1) we use a switch to discern the event type. It is easy to forget a return or a break. If we passed more data along, the signature for Switch would likely change to void Switch(EventType evt, void* data). That\u0026rsquo;s definitely bad. In (2), we cannot mistake the types, and data can easily be passed inside the event structs. Sadly, the events are not different types, and C++ does not allow for heterogeneous containers. Or does it?\nEnter C++17\u0026rsquo;s ✨std::variant✨.\nWhat is std::variant, you ask? Well, it is a union. A safe union! Safe meaning that you can only get a value from it if it really is there. No more type confusion; no more casting void pointers. But how do we get the values out of std::variant? Using std::visit, of course! Compilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #include \u0026lt;variant\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;cstdio\u0026gt; struct EventA {}; // (1) struct EventB {}; struct EventHandler { // (2) void operator() (EventA) { puts(\u0026#34;A\u0026#34;); } void operator() (EventB) { puts(\u0026#34;B\u0026#34;); } }; using event_t = std::variant\u0026lt;EventA, EventB\u0026gt;; int main() { std::vector\u0026lt;event_t\u0026gt; events = {EventA{}, EventB{}}; for (auto\u0026amp; event : events) { std::visit(EventHandler{}, event); // (3) } return 0; } In (1) we define our new event types. This time they are different types, not just different values. In (2) we define our event handler. All we need is an function overload for every possible type, and a struct with multiple operator() methods is an easy way to do it. Now all that is left to do is call std::visit with our handler and an event. If we forget to handle an event type - the code does not compile! This way, we know that we always handle all event types, and never mix them up.\nNow, if you liked the previous part, you probably don\u0026rsquo;t like writing a different handler class for every function. It completely ruins the locality of the code. But, we are using C++17, aren\u0026rsquo;t we? Compilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include \u0026lt;variant\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;cstdio\u0026gt; template\u0026lt;class... Ts\u0026gt; struct overloaded : Ts... { using Ts::operator()...; }; // (1) template\u0026lt;class... Ts\u0026gt; overloaded(Ts...) -\u0026gt; overloaded\u0026lt;Ts...\u0026gt;; struct EventA {}; struct EventB {}; using event_t = std::variant\u0026lt;EventA, EventB\u0026gt;; int main() { std::vector\u0026lt;event_t\u0026gt; events = {EventA{}, EventB{}}; for (auto\u0026amp; event : events) { std::visit(overloaded { // (2) [] (EventA) { puts(\u0026#34;A\u0026#34;); }, [] (EventB) { puts(\u0026#34;B\u0026#34;); } }, event); } return 0; } If you\u0026rsquo;re not familiar with C++17, there may be a lot to take in here. In (1) we define a class that takes multiple lambdas and overloads them. In (2) we instantiate that class to inline our event handling functions. The full explanation to this code is a bit long, so I wrote another post to explain it.\nApplied to the state-machine, it will look like this: Compilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 #include \u0026lt;tuple\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;variant\u0026gt; template\u0026lt;class... Ts\u0026gt; struct overloaded : Ts... { using Ts::operator()...; }; template\u0026lt;class... Ts\u0026gt; overloaded(Ts...) -\u0026gt; overloaded\u0026lt;Ts...\u0026gt;; struct EventA {}; struct EventB {}; using Event = std::variant\u0026lt;EventA, EventB\u0026gt;; struct Context { Context Inc() const { return Context{counter + 1}; } int counter = 0; }; template \u0026lt;class T\u0026gt; struct identity { using type = T; }; template \u0026lt;class T\u0026gt; using identity_t = typename identity\u0026lt;T\u0026gt;::type; template \u0026lt;template \u0026lt;class T\u0026gt; class Base = identity_t, class... Args\u0026gt; struct SelfReturning { using RetType = Base\u0026lt;SelfReturning\u0026gt;; using FuncType = RetType(*)(Args... args); SelfReturning(FuncType func) : _func{ func } {} RetType operator() (Args... args) { return _func(std::forward\u0026lt;Args\u0026gt;(args)...); } FuncType _func; template \u0026lt;class... AltArgs\u0026gt; using WithArgs = SelfReturning\u0026lt;Base, AltArgs...\u0026gt;; }; template \u0026lt;class T\u0026gt; using PairWithCtx = std::pair\u0026lt;T, const Context\u0026gt;; using State = SelfReturning\u0026lt;PairWithCtx\u0026gt;::WithArgs\u0026lt;const Context\u0026amp;, Event\u0026gt;; State::RetType A(const Context\u0026amp;, Event); State::RetType B(const Context\u0026amp;, Event); State::RetType A(const Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State A, counter = %d\\n\u0026#34;, ctx.counter); return std::visit(overloaded{ [\u0026amp;] (EventA) { return make_pair(A, ctx); }, [\u0026amp;] (EventB) { return make_pair(B, ctx.Inc()); } }, evt); } State::RetType B(const Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State B, counter = %d\\n\u0026#34;, ctx.counter); return std::visit(overloaded{ [\u0026amp;] (EventA) { return make_pair(A, ctx.Inc()); }, [\u0026amp;] (EventB) { return make_pair(B, ctx); } }, evt); } int main() { State state = A; Context ctx{}; Event events[] = {EventB{}, EventA{}, EventB{}, EventA{}, }; for (auto evt : events) { std::tie(state, ctx) = state(ctx, evt); } return 0; } As you can see, the change is minimal.\nPassing In Data With that, it is time to address an issue I completely neglected in Part 1. Passing in data.\nOur current state-machine model is based on the idea that the events themselves are the only information the states need. This is naive. In many real-life scenarios, events carry data with them. Now, with std::variant, we can puts data into the different event types. All we need to do is add data-members to our event structs. We define our new, data-carrying events as follows:\n1 2 3 4 5 6 struct EventA { const char* msg{nullptr}; }; struct EventB { int number{0}; }; Nothing else needs to change. And now, in the state functions, we can easily access the event data:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 State::RetType A(const Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State A, counter = %d\\n\u0026#34;, ctx.counter); return std::visit(overloaded{ [\u0026amp;] (EventA e) { if (e.msg != nullptr) { printf(\u0026#34;A message = \\\u0026#34;%s\\\u0026#34;\u0026#34;, e.msg); } else { puts(\u0026#34;A message = nullptr\u0026#34;); } return make_pair(A, ctx); }, [\u0026amp;] (EventB) { return make_pair(B, ctx.Inc()); } }, evt); } State::RetType B(const Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State B, counter = %d\\n\u0026#34;, ctx.counter); return std::visit(overloaded{ [\u0026amp;] (EventA e) { return make_pair(A, ctx.Inc()); }, [\u0026amp;] (EventB e) { printf(\u0026#34;B number = %d\\n\u0026#34;, e.number); return make_pair(B, ctx); } }, evt); } Et voilà.\nPutting everything together now, we get the following code:\nCompilation,Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 #include \u0026lt;tuple\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;variant\u0026gt; template\u0026lt;class... Ts\u0026gt; struct overloaded : Ts... { using Ts::operator()...; }; template\u0026lt;class... Ts\u0026gt; overloaded(Ts...) -\u0026gt; overloaded\u0026lt;Ts...\u0026gt;; struct EventA { const char* msg{nullptr}; }; struct EventB { int number{0}; }; using Event = std::variant\u0026lt;EventA, EventB\u0026gt;; struct Context { Context Inc() const { return Context{counter + 1}; } int counter = 0; }; template \u0026lt;class T\u0026gt; struct identity { using type = T; }; template \u0026lt;class T\u0026gt; using identity_t = typename identity\u0026lt;T\u0026gt;::type; template \u0026lt;template \u0026lt;class T\u0026gt; class Base = identity_t, class... Args\u0026gt; struct SelfReturning { using RetType = Base\u0026lt;SelfReturning\u0026gt;; using FuncType = RetType(*)(Args... args); SelfReturning(FuncType func) : _func{ func } {} RetType operator() (Args... args) { return _func(std::forward\u0026lt;Args\u0026gt;(args)...); } FuncType _func; template \u0026lt;class... AltArgs\u0026gt; using WithArgs = SelfReturning\u0026lt;Base, AltArgs...\u0026gt;; }; template \u0026lt;class T\u0026gt; using PairWithCtx = std::pair\u0026lt;T, const Context\u0026gt;; using State = SelfReturning\u0026lt;PairWithCtx\u0026gt;::WithArgs\u0026lt;const Context\u0026amp;, Event\u0026gt;; State::RetType A(const Context\u0026amp;, Event); State::RetType B(const Context\u0026amp;, Event); State::RetType A(const Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State A, counter = %d\\n\u0026#34;, ctx.counter); return std::visit(overloaded{ [\u0026amp;] (EventA e) { if (e.msg != nullptr) { printf(\u0026#34;A message = \\\u0026#34;%s\\\u0026#34;\u0026#34;, e.msg); } else { puts(\u0026#34;A message = nullptr\u0026#34;); } return make_pair(A, ctx); }, [\u0026amp;] (EventB) { return make_pair(B, ctx.Inc()); } }, evt); } State::RetType B(const Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State B, counter = %d\\n\u0026#34;, ctx.counter); return std::visit(overloaded{ [\u0026amp;] (EventA e) { return make_pair(A, ctx.Inc()); }, [\u0026amp;] (EventB e) { printf(\u0026#34;B number = %d\\n\u0026#34;, e.number); return make_pair(B, ctx); } }, evt); } int main() { State state = A; Context ctx{}; Event events[] = {EventB{}, EventA{}, EventB{}, EventB{10}, EventA{}, EventA{\u0026#34;Hello, world!\u0026#34;}}; for (auto evt : events) { std::tie(state, ctx) = state(ctx, evt); } return 0; } Summary As promised, we have used some dark template magic to achieve:\nA nice generalization of SelfReturning, allowing customization of both return types and argument types; Better compile-time safety by replacing the switch statement with overload resolution; Passing data along with the events. Hopefully, a lot of fun along the way. ","permalink":"https://tamir.dev/posts/a-functional-style-state-machine-in-cpp-part-2/","summary":"First, an apology. The first part of this post was published on May 26. It is now September. I had most of the code for this part done by then. But finalizing the code took some more effort. Once that was done, explaining took a while. There were quite a few things I had to learn myself first. So now, months later, I present this humble offering to the Gods of C++ and template meta-programming.","title":"A Functional-Style State Machine in C++, Part 2"},{"content":"C++17 has granted us with std::variant. Simply put, it is a type-safe union. To access the value it stores, you can either request a specific type (using std::get or something similar) or \u0026ldquo;visit\u0026rdquo; the variant, automatically handling only the data-type that is actually there. Visiting is done using std::visit, and is fairly straight forward.\nCompilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #include \u0026lt;variant\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;vector\u0026gt; using var_t = std::variant\u0026lt;int, const char*\u0026gt;; // (1) struct Print { // (2) void operator() (int i) { printf(\u0026#34;%d\\n\u0026#34;, i); } void operator () (const char* str) { puts(str); } }; int main() { std::vector\u0026lt;var_t\u0026gt; vars = {1, 2, \u0026#34;Hello, World!\u0026#34;}; // (3) for (auto\u0026amp; v : vars) { std::visit(Print{}, v); // (4) } return 0; } In (1) we define our variant type. In (2) we define a class with an overloaded operator(). This is needed for the call to std::visit. In (3) we define a vector of variants. In (4) we visit each variant. We pass in an instance of Print, and overload resolution ensures that the correct overload will be called for every type. But this example forces us to write and name an object for the overloaded operator(). We can do better. In fact, the example for std::visit on cppreference already does. Here is an example derived from it:\nCompilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 #include \u0026lt;variant\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;vector\u0026gt; template\u0026lt;class... Ts\u0026gt; struct overloaded : Ts... { using Ts::operator()...; }; // (1) template\u0026lt;class... Ts\u0026gt; overloaded(Ts...) -\u0026gt; overloaded\u0026lt;Ts...\u0026gt;; // (2) using var_t = std::variant\u0026lt;int, const char*\u0026gt;; int main() { std::vector\u0026lt;var_t\u0026gt; vars = {1, 2, \u0026#34;Hello, World!\u0026#34;}; for (auto\u0026amp; v : vars) { std::visit(overloaded { // (3) [](int i) { printf(\u0026#34;%d\\n\u0026#34;, i); }, [](const char* str) { puts(str); } }, v); } return 0; } This is certainly more compact, and we removed the Print struct. But how does it work? You can see a class-template (1), lambdas passed in as arguments for the construction (3), and something with an arrow and some more template magic (2). Let\u0026rsquo;s build it step by step.\nFirst, we want to break the print functions out of Print and compose them later.\nCompilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 struct PrintInt { //(1) void operator() (int i) { printf(\u0026#34;%d\\n\u0026#34;, i); } }; struct PrintCString { // (2) void operator () (const char* str) { puts(str); } }; struct Print : PrintInt, PrintCString { // (3) using PrintInt::operator(); using PrintCString::operator(); }; In (1) and (2), we define the same operators as before, but in separate structs. In (3), we are inherit from both of those structs, then explicitly use their operator(). This results in exactly the same results as before. Next, we convert Print into a class template. I\u0026rsquo;ll jump ahead and convert it directly to a variadic template.\nCompilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 template \u0026lt;class... Ts\u0026gt; // (1) struct Print : Ts... { using Ts::operator()...; }; int main() { std::vector\u0026lt;var_t\u0026gt; vars = {1, 2, \u0026#34;Hello, World!\u0026#34;}; for (auto\u0026amp; v : vars) { std::visit(Print\u0026lt;PrintCString, PrintInt\u0026gt;{}, v); // (2) } return 0; } In (1) we define the template. We take an arbitrary number of classes, inherit from them, and use their operator(). In (2) we instantiate the Print class-template with PrintCString and PrintInt to get their functionality. Next, we want to use lambdas to do the same. This is possible because lambdas are not functions; they are objects implementing operator().\nCompilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 int main() { std::vector\u0026lt;var_t\u0026gt; vars = {1, 2, \u0026#34;Hello, World!\u0026#34;}; auto PrintInt = [](int i) { printf(\u0026#34;%d\\n\u0026#34;, i); }; // (1) auto PrintCString = [](const char* str) { puts(str); }; for (auto\u0026amp; v : vars) { std::visit( Print\u0026lt;decltype(PrintCString), decltype(PrintInt)\u0026gt;{PrintCString, PrintInt}, // (2) v); } return 0; } In (1) we define the lambdas we need. In (2) we instantiate the template with our lambdas. This is ugly. Since lambdas have unique types, we need to define them before using them as template parameters (deducing their types using decltype). Then, we need to pass the lambdas as arguments for aggregate initialization as lambdas have a delete default constructor. We are close, but not quite there yet. The \u0026lt;decltype(PrintCString), decltype(PrintInt)\u0026gt; part is really ugly, and causes repetition. But it is needed as ctors cannot do type-deduction. So in proper C++ style, we will create a function to circumvent that.\nCompilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 template \u0026lt;class... Ts\u0026gt; // (1) auto MakePrint(Ts... ts) { return Print\u0026lt;Ts...\u0026gt;{ts...}; } int main() { std::vector\u0026lt;var_t\u0026gt; vars = {1, 2, \u0026#34;Hello, World!\u0026#34;}; for (auto\u0026amp; v : vars) { std::visit( MakePrint( // (2) [](const char* str) { puts(str); }, [](int i) { printf(\u0026#34;%d\\n\u0026#34;, i); } ), v); } return 0; } In (1) we define our helper function, to perform type deduction and forward it to the ctor. In (2) we take advantage of our newly found type-deduction to define the lambdas inline. But this is C++17, and we can do better.\nC++17 added user-defined deduction guides. Those allow us to instruct the compiler to perform the same actions as our helper function, but without adding another function. Using a suitable deduction guide, the code is as follows.\nCompilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #include \u0026lt;variant\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;vector\u0026gt; using var_t = std::variant\u0026lt;int, const char*\u0026gt;; template \u0026lt;class... Ts\u0026gt; struct Print : Ts... { using Ts::operator()...; }; template \u0026lt;class...Ts\u0026gt; Print(Ts...) -\u0026gt; Print\u0026lt;Ts...\u0026gt;; // (1) int main() { std::vector\u0026lt;var_t\u0026gt; vars = {1, 2, \u0026#34;Hello, World!\u0026#34;}; for (auto\u0026amp; v : vars) { std::visit( Print{ // (2) [](const char* str) { puts(str); }, [](int i) { printf(\u0026#34;%d\\n\u0026#34;, i); } }, v); } return 0; } In (1) we define a deduction guide which acts as our previous helper function, and in (2) we use the constructor instead of a helper function. Done.\nNow we have fully recreated the original example. As Print is no longer indicative of the template-class\u0026rsquo; behavior, overloaded is probably a better name.\n1 2 template\u0026lt;class... Ts\u0026gt; struct overloaded : Ts... { using Ts::operator()...; }; template\u0026lt;class... Ts\u0026gt; overloaded(Ts...) -\u0026gt; overloaded\u0026lt;Ts...\u0026gt;; ","permalink":"https://tamir.dev/posts/that-overloaded-trick-overloading-lambdas-in-cpp17/","summary":"C++17 has granted us with std::variant. Simply put, it is a type-safe union. To access the value it stores, you can either request a specific type (using std::get or something similar) or \u0026ldquo;visit\u0026rdquo; the variant, automatically handling only the data-type that is actually there. Visiting is done using std::visit, and is fairly straight forward.\nCompilation, Execution\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #include \u0026lt;variant\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;vector\u0026gt; using var_t = std::variant\u0026lt;int, const char*\u0026gt;; // (1) struct Print { // (2) void operator() (int i) { printf(\u0026#34;%d\\n\u0026#34;, i); } void operator () (const char* str) { puts(str); } }; int main() { std::vector\u0026lt;var_t\u0026gt; vars = {1, 2, \u0026#34;Hello, World!","title":"That `overloaded` Trick: Overloading Lambdas in C++17"},{"content":"If you go to any of your colleagues now and ask them, \u0026ldquo;can a function in C++ return itself?\u0026rdquo; they will probably give you the wrong answer. Now ask them what the return type of the function is going to be. Here, let me help you:\n1 2 3 using SelfReturning = SelfReturning (*)(); SelfReturning A() { return A; } Great! But it doesn\u0026rsquo;t compile. and neither does\n1 auto A() { return A; } It turns out that C++\u0026rsquo;s type-system does not allow for recursive types. This is annoying. There is no reason why a function should not be able to return itself. It is even more annoying given that object methods can return the objects that hold them:\n1 2 3 struct A { A operator()() { return A(); } }; This code works. And for obvious reasons. Object methods are not a part of the object. They do not affect the object size or its construction. They are just a syntactic utility. There is no type-system recursion going on here.\nWith functions there is obvious type-recursion. But if you look at the work the compiler actually has to do - it seems absurd. A function is never constructed, it just is. It is not allocated. The function\u0026rsquo;s signature changes nothing about the type.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 void* A() { return reinterpret_cast\u0026lt;void*\u0026gt;(A); // Same as C\u0026#39;s `(void*)A` } int main() { auto a = A; while (true) { // Cast back to function pointer a = reinterpret_cast\u0026lt;void*(*)()\u0026gt;(A()); } return 0; } See? No missing information. The compiler has all the knowledge it needs, but the type-system still prevents us from writing our code (or, in this case, from writing it in a type-safe manner). We can do better.\nWe already know that objects can be used to break type recursion. Let\u0026rsquo;s see if we can use them here without creating so much boiler-plate code:\n1 2 3 4 5 6 7 8 9 struct SelfReturning { using FuncType = SelfReturning(*)(); // (1) SelfReturning(FuncType func) : _func{func} {} // (2) SelfReturning operator() () { return _func(); } // (3) private: FuncType _func; }; The answer is yes. We can. Just substitute this class for the failed type definition of the first example and everything works as advertised. But how does it work? To break the type-recursion, we create a proxy object. Its sole purpose is to hold a function pointer and call it. Line (1) defines the function type that we expect to hold. Note that there is no direct recursion there. (2) is the constructor, taking the function pointer and storing it. (3) is where we forward the call to the function pointer. Note that here, too there is no type recursion as the type of the class is distinct from the type of its operator() function. As a bonus, this compiles identical to the reinterpret_cast\u0026lt;void*\u0026gt; version in both Clang and GCC when using -O3 (see here and here), and at the same time maintaining type-safety. Zero-cost abstraction at work.\nBut why is that interesting? What are the use-cases?\nWell, during the last few months, I\u0026rsquo;ve routinely consumed one programming-related talk per day. I find it a great way to expand my knowledge, and far easier to do than reading an article every day.\nLast week, while working on some minor state-machine, I came across Declarative Thinking, Declarative Practice by Kevlin Henney. Upon seeing this slide:\nI thought - bare functions instead of the State design pattern? I have to try that! So I went ahead and wrote my code, iterating through the steps described above. At a quick glance, the functor solution may seem satisfying. But in effect functors, unlike functions, have different types and cannot be assigned to the same variable. To bridge the gap, we use an abstract base-class and polymorphism. Once we do that, we are forced to use pointers to hold the states. I use std::unique_ptr as I don\u0026rsquo;t want to manage the memory myself.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #include \u0026lt;memory\u0026gt; struct IState { virtual std::unique_ptr\u0026lt;IState\u0026gt; operator()() = 0; virtual ~IState() {}; }; struct A : public IState { std::unique_ptr\u0026lt;IState\u0026gt; operator()(); }; struct B : public IState { std::unique_ptr\u0026lt;IState\u0026gt; operator()(); }; std::unique_ptr\u0026lt;IState\u0026gt; A::operator()() { return std::make_unique\u0026lt;B\u0026gt;(); } std::unique_ptr\u0026lt;IState\u0026gt; B::operator()() { return std::make_unique\u0026lt;A\u0026gt;(); } int main() { std::unique_ptr\u0026lt;IState\u0026gt; state = std::make_unique\u0026lt;A\u0026gt;(); while (true) { state = (*state)(); } return 0; } The proxy-object trick, however, has no such overhead. We know that we are using objects, but the code does not show it. The compiled version is far simpler as well (see here and here).\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 struct State { using FuncType = State(*)(); State(FuncType func) : _func{func} {}; State operator()() { return _func(); } FuncType _func; }; State A(); State B(); State A() { return B; } State B() { return A; } int main() { State state = A; while (true) { state = state(); } return 0; } Enhancing it a bit, to handle events and operate on a context, we still maintain very simple, straight-forward code. For the purpose of this example, abort() and printf() are used instead of throw std::runtime_error and std::cout because the compiled output is easier to read. See compilation here and execution here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cstdlib\u0026gt; enum class Event{ A, B, }; struct Context { int counter = 0; }; struct State { using FuncType = State(*)(Context\u0026amp;, Event); State(FuncType func) : _func{func} {}; State operator()(Context\u0026amp; ctx, Event evt) { return _func(ctx, evt); } FuncType _func; }; State A(Context\u0026amp;, Event); State B(Context\u0026amp;, Event); State A(Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State A, counter = %d\\n\u0026#34;, ctx.counter); ++ctx.counter; switch (evt) { case Event::A : return A; case Event::B : return B; default: abort(); } } State B(Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State B, counter = %d\\n\u0026#34;, ctx.counter); ++ctx.counter; switch (evt) { case Event::A : return A; case Event::B : return B; default: abort(); } } int main() { State state = A; Context ctx{}; Event events[] = {Event::B, Event::A, Event::B, Event::A, }; for (auto evt : events) { state = state(ctx, evt); } return 0; } For those keen on functional programming, we can even pass in a const reference to the context, and return a new context along with the new state. Compilation, execution.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 #include \u0026lt;tuple\u0026gt; #include \u0026lt;cstdio\u0026gt; #include \u0026lt;cstdlib\u0026gt; enum class Event{ A, B, }; struct Context { Context Inc() const { return Context{counter + 1}; } int counter = 0; }; struct State { using RetType = std::pair\u0026lt;State, const Context\u0026gt;; using FuncType = RetType(*)(const Context\u0026amp;, Event); State(FuncType func) : _func{func} {}; RetType operator()(Context\u0026amp; ctx, Event evt) { return _func(ctx, evt); } FuncType _func; }; State::RetType A(const Context\u0026amp;, Event); State::RetType B(const Context\u0026amp;, Event); State::RetType A(const Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State A, counter = %d\\n\u0026#34;, ctx.counter); switch (evt) { case Event::A : return {A, ctx}; case Event::B : return {B, ctx.Inc()}; default: abort(); } } State::RetType B(const Context\u0026amp; ctx, Event evt) { printf(\u0026#34;State B, counter = %d\\n\u0026#34;, ctx.counter); switch (evt) { case Event::A : return {A, ctx.Inc()}; case Event::B : return {B, ctx}; default: abort(); } } int main() { State state = A; Context ctx{}; Event events[] = {Event::B, Event::A, Event::B, Event::A, }; for (auto evt : events) { std::tie(state, ctx) = state(ctx, evt); } return 0; } And that\u0026rsquo;s it. We have a state machine based on pure-, bare-functions, in C++. It has a nice, simple look to it and as we\u0026rsquo;ve seen, compiles into far simpler code than the alternatives. On the way, we\u0026rsquo;ve also learned a bit about C++\u0026rsquo;s type system and how to use objects to overcome its limitations. In the next post (soon to be populated now online!) I will show some exciting (read: never use in production) dark template magic to both generalize the State object and to get some compile-time guarantees. Stay tuned!\n","permalink":"https://tamir.dev/posts/a-functional-style-state-machine-in-cpp/","summary":"If you go to any of your colleagues now and ask them, \u0026ldquo;can a function in C++ return itself?\u0026rdquo; they will probably give you the wrong answer. Now ask them what the return type of the function is going to be. Here, let me help you:\n1 2 3 using SelfReturning = SelfReturning (*)(); SelfReturning A() { return A; } Great! But it doesn\u0026rsquo;t compile. and neither does\n1 auto A() { return A; } It turns out that C++\u0026rsquo;s type-system does not allow for recursive types.","title":"A Functional-Style State Machine in C++"},{"content":"TL;DR Windows\u0026rsquo; shell sucks so people write tools. Tools are fun to use.\nLinux\u0026rsquo;s shell is amazing so people write terrible bash scripts and makefiles.\nBe sensible. Use Python. Use C.\nThe rest of the post is me ranting, letting off some steam. Have fun, and don\u0026rsquo;t take it too seriously.\n🐢 A Story of Shells 🐢 1 2 3 my_var=$(application arg0 arg1) for /f %%i in (\u0026#39;application arg0 arg1\u0026#39;) do set my_var=%%i 🪟 Windows 🪟 I\u0026rsquo;ve been a Windows user for a very long time. In fact, it has always been my main OS. And I absolutely love it. That is not to say, though, that there aren\u0026rsquo;t issues with that. One such thing is the CLI.\nFor years, the only thing we had on Windows was cmd.exe and .bat files. And they are ghastly. They are pretty straight-forward for very simple things, but try using them for anything more advanced (like setting a variable to the output of a command) and you\u0026rsquo;ll quickly realize that this is not the tool for you. You need a real programming language. So you\u0026rsquo;ll use Python, or C, or something. It will take you a bit longer to write the tool you needed, but in the process it will become an actual tool and not an annoying script.\nIn recent years there has been some serious improvements - PowerShell came along, cmder made the shell look a bit better, and I have some Linux tools (grep, awk, xargs\u0026hellip;) running in my Windows shell. And yet, people tend to use more general purpose languages on Windows.\n🐧 Penguins 🐧 On Linux, however, things have always been good with the shell. It has awesome terminals, and a huge amount of utilities that can be chained (piped) to unleash powers beyond imagination. And unlike Windows, where you need to use funny-looking APIs to get information, Linux just gives you everything in handy text files.\nIn fact, Linux is optimized for the shell in ways that make me wince. It is actually easier to parse system information (like, say, a process list) in the shell than in C code. Because instead of proper APIs, we have text files. Text files. The kernel takes binary data, formats it into strings, and the user-mode code can then scanf the code back into binary data. Amazing!\nBut enough of that.\nThe Linux shell is truly remarkable. You can do pretty much anything in a bash script. And people do. Now, there\u0026rsquo;s a bit of Linux philosophy that I really like. \u0026ldquo;Do one thing and do it well.\u0026rdquo; find finds things, grep greps, xargs xargs, and awk awks. So far - so good. But what about bash itself? While each of the shell utilities does one thing and does it well, bash, and especially bash scripts, do not. They allow you to quickly implement advanced behaviours by hacking together multiple commands. You write more and more and more, and everything just works. And then, it doesn\u0026rsquo;t. And you need to fix it. If you\u0026rsquo;re lucky, and the code is well documented you might get away with that. But more often than not, it won\u0026rsquo;t be. And the wonderful \u0026ldquo;let\u0026rsquo;s take strings and pass them around\u0026rdquo; programming style might come back to bite you.\nBut, again, bash is fantastic for quick hacks. And works fairly well in general when you\u0026rsquo;re not taking input parameters, and when it\u0026rsquo;s small enough. I don\u0026rsquo;t like it, but it works.\n🔥 Make Your Own Hell 🔥 The real issue is make.\nmake is a beast spawned in the deepest dungeons of hell (and I am awfully sorry if I offended any such beast by the comparison.) Makefiles give you the benefits of never leaving your shell / code-editor while you work. You just create a makefile, define your targets, and you\u0026rsquo;re good to go. In truth - that is fantastic. You can even include bash scripts in your makefiles to do custom steps. Or generate makefiles. Or use automake. Or anything else that might make the task of writing makefiles easier to do and harder to maintain. But you write, and you specify, and it just works! Most of the time.\n** Dramatic Pause **\nYou might have noticed I don\u0026rsquo;t like make. That\u0026rsquo;s true. This post is mainly me complaining about make and blowing off some steam. Now, I appreciate make. I\u0026rsquo;ve used it to build things that I never would\u0026rsquo;ve managed on Windows. It\u0026rsquo;s an extremely powerful tool. But, you see, as a Windows user, I naturally enjoy IDEs. One such wonderful creation is Visual Studio. As far as C/C++ development tools go it is unparalleled. \u0026ldquo;But wait!\u0026rdquo; say my Linux friends, \u0026ldquo;VS forces you to create foul \u0026lsquo;projects\u0026rsquo; using their fiendish \u0026lsquo;GUI\u0026rsquo;! In Linux, we just write makefiles!\u0026rdquo; That is true, and it is quite annoying with small projects (where I usually end up having a .bat file to trigger the build with all the relevant flags,) but it is godsend for anything more complex. You have GUI, and Projects, and Solutions and whatnot. It\u0026rsquo;s great. A bit slow to define, but oh so easy to use!\nAnd now, after I lost most of my Linux-oriented readers, I can get to the point. Linux has a super-powerful shell, so people use it to make a super-powerful mess. Windows has a super-useless shell, so people don\u0026rsquo;t use it. Instead - they build tools! User-friendly(-ish) tools with GUI. And they use general purpose programming languages. And that\u0026rsquo;s good.\n🍕 Takeaway 🍕 Should you stop using bash? Or cripple it? Or keep Windows\u0026rsquo; shell down? Hell no!\nUse whatever tools you deem fit. But use them wisely. Keep shell-scripting to small automation tasks, and try to use more maintainable programming languages when you write something larger or more complex. And do write new tools. Tools are fun.\n","permalink":"https://tamir.dev/posts/the-windows-cli-sucks-and-thats-good/","summary":"TL;DR Windows\u0026rsquo; shell sucks so people write tools. Tools are fun to use.\nLinux\u0026rsquo;s shell is amazing so people write terrible bash scripts and makefiles.\nBe sensible. Use Python. Use C.\nThe rest of the post is me ranting, letting off some steam. Have fun, and don\u0026rsquo;t take it too seriously.\n🐢 A Story of Shells 🐢 1 2 3 my_var=$(application arg0 arg1) for /f %%i in (\u0026#39;application arg0 arg1\u0026#39;) do set my_var=%%i 🪟 Windows 🪟 I\u0026rsquo;ve been a Windows user for a very long time.","title":"The Windows CLI sucks, and that's good."},{"content":"As git users, we know that we should \u0026ldquo;commit early, commit often.\u0026rdquo; While this is a wonderful thing to do, it does mean that from time to time we make a mistake and need to fix a commit. Maybe we forget to git add a new file, or missed a typo. So we go ahead and git commit --amend. Problem solved. Great.\nBut personally, I hate it.\nFor one thing, amending commits hides history. Once you amend, that past state is gone before you can properly test the new one. True, you can also restore it via git reflog, but no-one really likes using that. It should be a last resort.\nFor another thing, amending is very limited. Say I am writing some C code. I write my first module, add it and commit.\n1 2 git add FirstModule.h git commit -m \u0026#34;Added FirstModule\u0026#34; I write my second module, and add it as well.\n1 2 git add SecondModule.h SecondModule.c git commit -m \u0026#34;Added SecondModule\u0026#34; And now, after adding that second commit, I realize that I forgot to commit FirstModule.c. git commit --amend to the rescue? Not really. I now have to resort to the black, frightening voodoo magic called git rebase.\nFirst, we commit the forgotten module\n1 2 git add FirstModule.c git commit -m \u0026#34;Added FirstModule.c, forgotten eariler.\u0026#34; And then rebase - git rebase -i HEAD~3\n1 2 3 pick 1db8687 Added FirstModule pick 336941b Added SecondModule pick 7884909 Added FirstModule.c, forgotten eariler. Change to\n1 2 3 pick 1db8687 Added FirstModule fixup 7884909 Added FirstModule.c, forgotten eariler. pick 336941b Added SecondModule Save \u0026amp; Quit, and we\u0026rsquo;re done.\n1 2 3 4 5 6 7 * 1946e37d105ffebcbd91bb958f8a2fce6160c761 (HEAD -\u0026gt; master) Added SecondModule | create mode 100644 SecondModule.c | create mode 100644 SecondModule.h * 8ffbb9f2915e060a6c4771e13f5a82442743724c Added FirstModule | create mode 100644 FirstModule.c | create mode 100644 FirstModule.h * 815e7bab6ee1fa5bf1df10f5705919b48cbe214c First Commit Not that hard, is it?\nBut still, moving between amending and rebasing can be cumbersome. Especially as most of the time there is no real need to rebase and it\u0026rsquo;s easy to forget the process. Enter git commit --fixup (or --squash) and git rebase -i --autosquash.\nThese commands save us the work of reordering the commits and changing from pick to fixup or squash. Making our rebasing work a lot easier.\nI like defining the following aliases:\n1 2 3 4 5 [alias] ri = rebase -i --autosquash mri = rebase -i fix = commit --fixup squ = commit --squash Using those aliases, the rebasing we did earlier would work as follows:\n1 2 3 git add FirstModule.c git fix HEAD~1 git ri HEAD~3 We\u0026rsquo;d get the following rebase automatically\n1 2 3 pick 1db8687 Added FirstModule fixup 50a3650 fixup! Added FirstModule pick 336941b Added SecondModule Exit the editor, and be done with it.\nWe can use fix as many times as we want (just go ahead and git fix HEAD -a) before the rebase. Our log may look funny\n1 2 3 4 5 6 7 * fe0c2a0 (HEAD -\u0026gt; master) fixup! fixup! fixup! fixup! Added SecondModule * a53cd32 fixup! fixup! fixup! Added SecondModule * 9c19f2d fixup! fixup! Added SecondModule * b758a53 fixup! Added SecondModule * 902d65e Added SecondModule * 67f1260 Added FirstModule * 815e7ba First Commit But the rebase doesn\u0026rsquo;t care\n1 2 3 4 5 pick 902d65e Added SecondModule fixup b758a53 fixup! Added SecondModule fixup 9c19f2d fixup! fixup! Added SecondModule fixup a53cd32 fixup! fixup! fixup! Added SecondModule fixup fe0c2a0 fixup! fixup! fixup! fixup! Added SecondModule Conclusion Stop using git commit --amend and start using git fix (git commit --fixup) instead. It is a no-fear, low-overhead alternative, and it far more flexible. Here are the aliases again, in case you want them:\n1 2 3 4 5 [alias] ri = rebase -i --autosquash mri = rebase -i fix = commit --fixup squ = commit --squash ","permalink":"https://tamir.dev/posts/dont-amend-fix/","summary":"As git users, we know that we should \u0026ldquo;commit early, commit often.\u0026rdquo; While this is a wonderful thing to do, it does mean that from time to time we make a mistake and need to fix a commit. Maybe we forget to git add a new file, or missed a typo. So we go ahead and git commit --amend. Problem solved. Great.\nBut personally, I hate it.\nFor one thing, amending commits hides history.","title":"Don't Amend, Fix"}]